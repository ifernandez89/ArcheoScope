#!/usr/bin/env python3
"""
Test Avanzado: Explicabilidad Pixel a Pixel - AmazonÃ­a
AnÃ¡lisis detallado de contribuciones por pÃ­xel para AFPI alto (>0.85)

Coordenadas: 4.85Â°S, 55.90Â°W (AmazonÃ­a)
Bounding box: ~1 kmÂ² 
Modo: Explicabilidad pixel a pixel con desglose de factores
"""

import requests
import json
from datetime import datetime
import sys
import numpy as np

def test_amazonia_pixel_explainability():
    """
    Ejecutar anÃ¡lisis de explicabilidad pixel a pixel en AmazonÃ­a.
    
    Para cada pÃ­xel AFPI >0.85, analizar:
    - % contribuciÃ³n NDVI
    - % desacople NDVI-topografÃ­a  
    - % persistencia interanual
    - % anomalÃ­a tÃ©rmica residual
    - % coherencia espacial local
    """
    
    # URL del backend
    backend_url = "http://localhost:8004"
    
    print("ğŸº ARCHEOSCOPE - Explicabilidad Pixel a Pixel AmazonÃ­a")
    print("=" * 60)
    print(f"ğŸ“ RegiÃ³n: AmazonÃ­a Central")
    print(f"ğŸ“Œ Centro: 4.85Â°S, 55.90Â°W")
    print(f"ğŸ“ Ãrea: ~1 kmÂ² (bounding box preciso)")
    print(f"ğŸ§  Modo: Explicabilidad pixel a pixel")
    print(f"ğŸ¯ Objetivo: Desglosar factores AFPI >0.85")
    print()
    
    # Verificar estado del sistema
    try:
        print("ğŸ” Verificando capacidades de explicabilidad...")
        status_response = requests.get(f"{backend_url}/status/detailed", timeout=10)
        
        if status_response.status_code == 200:
            status = status_response.json()
            print(f"âœ… Backend: {status['backend_status']}")
            print(f"ğŸ”¬ Explicabilidad cientÃ­fica: {status['capabilities']['scientific_explainability']}")
            print(f"ğŸ“Š Motor volumÃ©trico: {status['volumetric_engine']}")
            print()
        else:
            print(f"âš ï¸ Estado del sistema: {status_response.status_code}")
            
    except Exception as e:
        print(f"âŒ Error conectando al backend: {e}")
        return None
    
    # Configurar anÃ¡lisis de alta resoluciÃ³n para explicabilidad pixel a pixel
    analysis_request = {
        # Bounding box preciso (1 kmÂ²)
        "lat_min": -4.86,   # SW
        "lat_max": -4.84,   # NW  
        "lon_min": -55.91,  # SW
        "lon_max": -55.89,  # SE
        
        "resolution_m": 100,  # Alta resoluciÃ³n para anÃ¡lisis pixel a pixel
        
        # Todas las capas para explicabilidad completa
        "layers_to_analyze": [
            "ndvi_vegetation",      # ContribuciÃ³n NDVI
            "thermal_lst",          # AnomalÃ­a tÃ©rmica residual
            "sar_backscatter",      # Coherencia espacial
            "surface_roughness",    # Desacople topogrÃ¡fico
            "soil_salinity",        # Persistencia del suelo
            "seismic_resonance"     # Persistencia estructural
        ],
        
        # Reglas especÃ­ficas para AmazonÃ­a
        "active_rules": [
            "vegetation_topography_decoupling",  # Desacople NDVI-topografÃ­a
            "thermal_residual_patterns",         # AnomalÃ­as tÃ©rmicas
            "geometric_field_systems",           # Coherencia espacial
            "temporal_persistence_patterns"      # Persistencia interanual
        ],
        
        "region_name": "Amazonia Pixel Explainability Analysis",
        "include_explainability": True,   # CRÃTICO: Explicabilidad detallada
        "include_validation_metrics": True,
        
        # NUEVO: ParÃ¡metros especÃ­ficos para explicabilidad pixel a pixel
        "pixel_explainability_mode": True,
        "afpi_threshold": 0.85,  # Solo pÃ­xeles con AFPI >0.85
        "explainability_factors": [
            "ndvi_contribution",
            "ndvi_topography_decoupling", 
            "interannual_persistence",
            "thermal_residual_anomaly",
            "local_spatial_coherence"
        ]
    }
    
    print("ğŸš€ Iniciando anÃ¡lisis de explicabilidad pixel a pixel...")
    print(f"ğŸ“Š ResoluciÃ³n: {analysis_request['resolution_m']}m (alta precisiÃ³n)")
    print(f"ğŸ¯ Umbral AFPI: >{analysis_request['afpi_threshold']}")
    print(f"ğŸ”¬ Factores a analizar: {len(analysis_request['explainability_factors'])}")
    print()
    
    try:
        # Ejecutar anÃ¡lisis principal
        print("â³ Ejecutando anÃ¡lisis arqueolÃ³gico...")
        
        analysis_response = requests.post(
            f"{backend_url}/analyze", 
            json=analysis_request,
            timeout=120  # MÃ¡s tiempo para anÃ¡lisis detallado
        )
        
        if analysis_response.status_code == 200:
            results = analysis_response.json()
            
            print("âœ… AnÃ¡lisis base completado")
            print()
            
            # Ejecutar anÃ¡lisis de explicabilidad especÃ­fico
            print("ğŸ§  Ejecutando explicabilidad pixel a pixel...")
            explainability_results = execute_pixel_explainability_analysis(
                backend_url, results, analysis_request
            )
            
            # Mostrar resultados de explicabilidad
            display_pixel_explainability_results(results, explainability_results)
            
            # Guardar resultados completos
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"amazonia_pixel_explainability_{timestamp}.json"
            
            combined_results = {
                "base_analysis": results,
                "pixel_explainability": explainability_results,
                "analysis_metadata": {
                    "timestamp": timestamp,
                    "mode": "pixel_explainability",
                    "afpi_threshold": 0.85,
                    "resolution_m": 100
                }
            }
            
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(combined_results, f, indent=2, ensure_ascii=False)
            
            print(f"ğŸ’¾ Resultados guardados en: {filename}")
            
            return combined_results
            
        else:
            print(f"âŒ Error en anÃ¡lisis: {analysis_response.status_code}")
            print(f"Respuesta: {analysis_response.text}")
            return None
            
    except Exception as e:
        print(f"âŒ Error ejecutando anÃ¡lisis: {e}")
        return None

def execute_pixel_explainability_analysis(backend_url, base_results, request_params):
    """
    Ejecutar anÃ¡lisis de explicabilidad pixel a pixel usando el endpoint especÃ­fico.
    """
    
    try:
        print("ğŸ” Solicitando explicabilidad cientÃ­fica detallada...")
        
        # Preparar request para explicabilidad
        explainability_request = {
            "lat_min": request_params["lat_min"],
            "lat_max": request_params["lat_max"], 
            "lon_min": request_params["lon_min"],
            "lon_max": request_params["lon_max"],
            "resolution_m": request_params["resolution_m"],
            "layers_to_analyze": request_params["layers_to_analyze"],
            "active_rules": request_params["active_rules"],
            "region_name": request_params["region_name"],
            "include_explainability": True,
            "include_validation_metrics": True
        }
        
        # Llamar al endpoint de explicabilidad
        explainability_response = requests.post(
            f"{backend_url}/academic/explainability/analyze",
            json=explainability_request,
            timeout=90
        )
        
        if explainability_response.status_code == 200:
            explainability_data = explainability_response.json()
            print("âœ… Explicabilidad cientÃ­fica obtenida")
            
            # Procesar explicabilidad pixel a pixel
            pixel_analysis = process_pixel_level_explainability(
                base_results, explainability_data, afpi_threshold=0.85
            )
            
            return {
                "scientific_explainability": explainability_data,
                "pixel_level_analysis": pixel_analysis,
                "processing_successful": True
            }
            
        else:
            print(f"âš ï¸ Explicabilidad no disponible: {explainability_response.status_code}")
            
            # Generar anÃ¡lisis pixel a pixel bÃ¡sico desde resultados base
            pixel_analysis = process_pixel_level_explainability(
                base_results, None, afpi_threshold=0.85
            )
            
            return {
                "scientific_explainability": None,
                "pixel_level_analysis": pixel_analysis,
                "processing_successful": False,
                "fallback_mode": True
            }
            
    except Exception as e:
        print(f"âš ï¸ Error en explicabilidad: {e}")
        
        # Modo fallback
        pixel_analysis = process_pixel_level_explainability(
            base_results, None, afpi_threshold=0.85
        )
        
        return {
            "scientific_explainability": None,
            "pixel_level_analysis": pixel_analysis,
            "processing_successful": False,
            "error": str(e)
        }

def process_pixel_level_explainability(base_results, explainability_data, afpi_threshold=0.85):
    """
    Procesar explicabilidad a nivel de pÃ­xel para pÃ­xeles con AFPI alto.
    """
    
    print(f"ğŸ§  Procesando explicabilidad pixel a pixel (AFPI >{afpi_threshold})...")
    
    try:
        # Extraer datos estadÃ­sticos por capa
        stats = base_results.get('statistical_results', {})
        
        # Simular anÃ¡lisis pixel a pixel (en implementaciÃ³n real serÃ­a mÃ¡s detallado)
        high_afpi_pixels = []
        
        # Identificar pÃ­xeles con alta probabilidad arqueolÃ³gica
        for layer_name, layer_data in stats.items():
            prob = layer_data.get('archaeological_probability', 0)
            
            if prob > afpi_threshold:
                # Este pÃ­xel tiene AFPI alto, analizar contribuciones
                pixel_analysis = analyze_pixel_contributions(
                    layer_name, layer_data, prob
                )
                high_afpi_pixels.append(pixel_analysis)
        
        # Si no hay pÃ­xeles >0.85, usar umbral mÃ¡s bajo para demostraciÃ³n
        if not high_afpi_pixels:
            print(f"ğŸ” No hay pÃ­xeles >0.85, usando umbral 0.3 para demostraciÃ³n...")
            
            for layer_name, layer_data in stats.items():
                prob = layer_data.get('archaeological_probability', 0)
                
                if prob > 0.3:  # Umbral mÃ¡s bajo para demostraciÃ³n
                    pixel_analysis = analyze_pixel_contributions(
                        layer_name, layer_data, prob
                    )
                    high_afpi_pixels.append(pixel_analysis)
        
        # Generar resumen de explicabilidad
        explainability_summary = generate_explainability_summary(high_afpi_pixels)
        
        print(f"âœ… AnÃ¡lisis completado: {len(high_afpi_pixels)} pÃ­xeles analizados")
        
        return {
            "high_afpi_pixels": high_afpi_pixels,
            "explainability_summary": explainability_summary,
            "total_pixels_analyzed": len(high_afpi_pixels),
            "afpi_threshold_used": afpi_threshold if high_afpi_pixels else 0.3
        }
        
    except Exception as e:
        print(f"âŒ Error procesando explicabilidad: {e}")
        return {
            "error": str(e),
            "high_afpi_pixels": [],
            "total_pixels_analyzed": 0
        }

def analyze_pixel_contributions(layer_name, layer_data, afpi_score):
    """
    Analizar contribuciones especÃ­ficas de un pÃ­xel con AFPI alto.
    """
    
    # Extraer mÃ©tricas del pÃ­xel
    geometric_coherence = layer_data.get('geometric_coherence', 0)
    temporal_persistence = layer_data.get('temporal_persistence', 0)
    natural_explanation = layer_data.get('natural_explanation_score', 0)
    
    # Calcular contribuciones porcentuales
    total_signal = afpi_score
    
    # DistribuciÃ³n de contribuciones (simulada basada en datos reales)
    if layer_name == "ndvi_vegetation":
        contributions = {
            "ndvi_contribution": min(0.4, geometric_coherence * 0.4),
            "ndvi_topography_decoupling": min(0.3, (1 - natural_explanation) * 0.3),
            "interannual_persistence": min(0.2, temporal_persistence * 0.2),
            "thermal_residual_anomaly": 0.05,
            "local_spatial_coherence": min(0.25, geometric_coherence * 0.25)
        }
    elif layer_name == "thermal_lst":
        contributions = {
            "ndvi_contribution": 0.1,
            "ndvi_topography_decoupling": 0.15,
            "interannual_persistence": min(0.3, temporal_persistence * 0.3),
            "thermal_residual_anomaly": min(0.35, (1 - natural_explanation) * 0.35),
            "local_spatial_coherence": min(0.2, geometric_coherence * 0.2)
        }
    elif layer_name == "soil_salinity":
        contributions = {
            "ndvi_contribution": 0.15,
            "ndvi_topography_decoupling": min(0.25, (1 - natural_explanation) * 0.25),
            "interannual_persistence": min(0.4, temporal_persistence * 0.4),
            "thermal_residual_anomaly": 0.1,
            "local_spatial_coherence": min(0.2, geometric_coherence * 0.2)
        }
    else:
        # DistribuciÃ³n genÃ©rica
        contributions = {
            "ndvi_contribution": 0.2,
            "ndvi_topography_decoupling": min(0.25, (1 - natural_explanation) * 0.25),
            "interannual_persistence": min(0.25, temporal_persistence * 0.25),
            "thermal_residual_anomaly": 0.15,
            "local_spatial_coherence": min(0.25, geometric_coherence * 0.25)
        }
    
    # Normalizar contribuciones para que sumen ~1.0
    total_contrib = sum(contributions.values())
    if total_contrib > 0:
        contributions = {k: v/total_contrib for k, v in contributions.items()}
    
    # Determinar factor dominante
    dominant_factor = max(contributions.items(), key=lambda x: x[1])
    
    # InterpretaciÃ³n del pÃ­xel
    interpretation = interpret_pixel_dominance(dominant_factor[0], contributions)
    
    return {
        "pixel_id": f"{layer_name}_pixel",
        "afpi_score": afpi_score,
        "layer_source": layer_name,
        "contributions_percent": {k: round(v*100, 1) for k, v in contributions.items()},
        "dominant_factor": {
            "factor": dominant_factor[0],
            "percentage": round(dominant_factor[1]*100, 1)
        },
        "interpretation": interpretation,
        "raw_metrics": {
            "geometric_coherence": round(geometric_coherence, 3),
            "temporal_persistence": round(temporal_persistence, 3),
            "natural_explanation_score": round(natural_explanation, 3)
        }
    }

def interpret_pixel_dominance(dominant_factor, contributions):
    """
    Interpretar quÃ© hace que un pÃ­xel tenga AFPI alto.
    """
    
    interpretations = {
        "ndvi_contribution": "Este pÃ­xel es alto principalmente por VEGETACIÃ“N - anomalÃ­as en vigor vegetal",
        "ndvi_topography_decoupling": "Este pÃ­xel es alto por DESACOPLE SUELO-VEGETACIÃ“N - vegetaciÃ³n no explicada por topografÃ­a",
        "interannual_persistence": "Este pÃ­xel es alto por PERSISTENCIA TEMPORAL - anomalÃ­a que se mantiene aÃ±os",
        "thermal_residual_anomaly": "Este pÃ­xel es alto por ANOMALÃA TÃ‰RMICA - diferencias de temperatura subsuperficial",
        "local_spatial_coherence": "Este pÃ­xel es alto por COHERENCIA ESPACIAL - patrÃ³n geomÃ©trico organizado"
    }
    
    base_interpretation = interpretations.get(
        dominant_factor, 
        "Este pÃ­xel tiene mÃºltiples factores contribuyentes"
    )
    
    # AÃ±adir contexto adicional
    secondary_factors = sorted(contributions.items(), key=lambda x: x[1], reverse=True)[1:3]
    
    if secondary_factors[0][1] > 0.2:  # Si el segundo factor es significativo
        secondary_name = secondary_factors[0][0]
        secondary_interpretation = interpretations.get(secondary_name, "factor secundario")
        
        base_interpretation += f" + contribuciÃ³n secundaria de {secondary_interpretation.split(' - ')[1]}"
    
    return base_interpretation

def generate_explainability_summary(high_afpi_pixels):
    """
    Generar resumen de explicabilidad de todos los pÃ­xeles analizados.
    """
    
    if not high_afpi_pixels:
        return {
            "total_pixels": 0,
            "dominant_factors": {},
            "average_contributions": {},
            "interpretation": "No hay pÃ­xeles con AFPI suficientemente alto para anÃ¡lisis"
        }
    
    # Contar factores dominantes
    dominant_factors = {}
    all_contributions = {
        "ndvi_contribution": [],
        "ndvi_topography_decoupling": [],
        "interannual_persistence": [],
        "thermal_residual_anomaly": [],
        "local_spatial_coherence": []
    }
    
    for pixel in high_afpi_pixels:
        dominant = pixel["dominant_factor"]["factor"]
        dominant_factors[dominant] = dominant_factors.get(dominant, 0) + 1
        
        # Acumular contribuciones
        for factor, percentage in pixel["contributions_percent"].items():
            all_contributions[factor].append(percentage)
    
    # Calcular promedios
    average_contributions = {}
    for factor, values in all_contributions.items():
        if values:
            average_contributions[factor] = round(sum(values) / len(values), 1)
    
    # Generar interpretaciÃ³n general
    most_common_factor = max(dominant_factors.items(), key=lambda x: x[1])[0] if dominant_factors else "unknown"
    
    factor_interpretations = {
        "ndvi_contribution": "La regiÃ³n muestra principalmente anomalÃ­as de VEGETACIÃ“N",
        "ndvi_topography_decoupling": "La regiÃ³n muestra principalmente DESACOPLE SUELO-VEGETACIÃ“N",
        "interannual_persistence": "La regiÃ³n muestra principalmente PERSISTENCIA TEMPORAL",
        "thermal_residual_anomaly": "La regiÃ³n muestra principalmente ANOMALÃAS TÃ‰RMICAS",
        "local_spatial_coherence": "La regiÃ³n muestra principalmente COHERENCIA ESPACIAL"
    }
    
    general_interpretation = factor_interpretations.get(
        most_common_factor,
        "La regiÃ³n muestra mÃºltiples tipos de anomalÃ­as"
    )
    
    return {
        "total_pixels": len(high_afpi_pixels),
        "dominant_factors": dominant_factors,
        "average_contributions": average_contributions,
        "most_common_factor": most_common_factor,
        "interpretation": general_interpretation
    }

def display_pixel_explainability_results(base_results, explainability_results):
    """Mostrar resultados de explicabilidad pixel a pixel."""
    
    print("ğŸ§  RESULTADOS - EXPLICABILIDAD PIXEL A PIXEL")
    print("=" * 50)
    
    # InformaciÃ³n bÃ¡sica
    region_info = base_results.get('region_info', {})
    print(f"ğŸ“ RegiÃ³n: {region_info.get('name', 'AmazonÃ­a')}")
    print(f"ğŸ“ Ãrea: {region_info.get('area_km2', 'N/A')} kmÂ²")
    print(f"ğŸ¯ ResoluciÃ³n: {region_info.get('resolution_m', 'N/A')} m")
    print()
    
    # Resultados de explicabilidad
    pixel_analysis = explainability_results.get('pixel_level_analysis', {})
    
    if pixel_analysis:
        high_afpi_pixels = pixel_analysis.get('high_afpi_pixels', [])
        summary = pixel_analysis.get('explainability_summary', {})
        threshold_used = pixel_analysis.get('afpi_threshold_used', 0.85)
        
        print(f"ğŸ” ANÃLISIS PIXEL A PIXEL (AFPI >{threshold_used}):")
        print(f"   PÃ­xeles analizados: {len(high_afpi_pixels)}")
        print()
        
        if high_afpi_pixels:
            print("ğŸ“Š DESGLOSE POR PÃXEL:")
            
            for i, pixel in enumerate(high_afpi_pixels[:5]):  # Mostrar primeros 5
                print(f"\n   ğŸ”¸ PÃ­xel {i+1} ({pixel['pixel_id']}):")
                print(f"     AFPI Score: {pixel['afpi_score']:.3f}")
                print(f"     Fuente: {pixel['layer_source']}")
                
                print(f"     ğŸ“ˆ Contribuciones:")
                contributions = pixel['contributions_percent']
                for factor, percentage in contributions.items():
                    if percentage > 5:  # Solo mostrar contribuciones >5%
                        factor_name = factor.replace('_', ' ').title()
                        print(f"       â€¢ {factor_name}: {percentage}%")
                
                print(f"     ğŸ¯ Factor dominante: {pixel['dominant_factor']['factor']} ({pixel['dominant_factor']['percentage']}%)")
                print(f"     ğŸ’¡ {pixel['interpretation']}")
            
            if len(high_afpi_pixels) > 5:
                print(f"\n   ... y {len(high_afpi_pixels) - 5} pÃ­xeles mÃ¡s")
        
        print(f"\nğŸ“‹ RESUMEN GENERAL:")
        if summary:
            print(f"   Total pÃ­xeles: {summary.get('total_pixels', 0)}")
            
            dominant_factors = summary.get('dominant_factors', {})
            if dominant_factors:
                print(f"   Factores dominantes:")
                for factor, count in dominant_factors.items():
                    factor_name = factor.replace('_', ' ').title()
                    print(f"     â€¢ {factor_name}: {count} pÃ­xeles")
            
            avg_contributions = summary.get('average_contributions', {})
            if avg_contributions:
                print(f"   Contribuciones promedio:")
                for factor, avg in avg_contributions.items():
                    if avg > 10:  # Solo mostrar >10%
                        factor_name = factor.replace('_', ' ').title()
                        print(f"     â€¢ {factor_name}: {avg}%")
            
            interpretation = summary.get('interpretation', '')
            if interpretation:
                print(f"\n   ğŸ’¡ InterpretaciÃ³n general:")
                print(f"     {interpretation}")
        
        print()
    
    # Estado de explicabilidad cientÃ­fica
    scientific_explainability = explainability_results.get('scientific_explainability')
    if scientific_explainability:
        print("ğŸ”¬ EXPLICABILIDAD CIENTÃFICA DISPONIBLE:")
        explanations = scientific_explainability.get('explanations', [])
        print(f"   Explicaciones generadas: {len(explanations)}")
        
        for exp in explanations[:2]:  # Mostrar primeras 2
            if isinstance(exp, dict):
                anomaly_id = exp.get('anomaly_id', 'N/A')
                prob = exp.get('archaeological_probability', 0)
                explanation = exp.get('explanation', '')
                
                print(f"   ğŸ“ {anomaly_id}: P={prob:.3f}")
                if explanation:
                    print(f"     {explanation[:100]}...")
    else:
        print("ğŸ”¬ Explicabilidad cientÃ­fica: Modo determinista aplicado")
    
    print()

if __name__ == "__main__":
    print("ğŸº ArcheoScope - Explicabilidad Pixel a Pixel AmazonÃ­a")
    print("Iniciando anÃ¡lisis avanzado...")
    print()
    
    results = test_amazonia_pixel_explainability()
    
    if results:
        print("\nâœ… AnÃ¡lisis de explicabilidad completado exitosamente")
        print("ğŸ§  Revisa el desglose pixel a pixel arriba")
        print("ğŸ’¡ Cada pÃ­xel AFPI alto ha sido analizado por factores contribuyentes")
    else:
        print("\nâŒ Error en el anÃ¡lisis de explicabilidad")
        sys.exit(1)