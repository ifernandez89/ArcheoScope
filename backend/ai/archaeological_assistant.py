#!/usr/bin/env python3
"""
Asistente IA arqueol√≥gico para ArcheoScope usando OpenRouter/Ollama.

Especializado en explicar anomal√≠as espaciales desde perspectiva arqueol√≥gica
con razonamiento cient√≠fico riguroso.
"""

import requests
import json
from typing import Dict, List, Any, Optional
import logging
from dataclasses import dataclass
from datetime import datetime
import os
from dotenv import load_dotenv

# Cargar variables de entorno
load_dotenv('.env.local')

logger = logging.getLogger(__name__)

@dataclass
class ArchaeologicalExplanation:
    """Explicaci√≥n arqueol√≥gica estructurada."""
    explanation: str
    archaeological_interpretation: str
    confidence_assessment: str
    methodological_notes: str
    recommendations: List[str]
    limitations: str
    scientific_reasoning: str

class ArchaeologicalAssistant:
    """
    Asistente IA especializado en arqueolog√≠a remota.
    
    Usa OpenRouter (Gemini 3 Preview) o Ollama para generar explicaciones 
    cient√≠ficamente rigurosas de anomal√≠as espaciales desde perspectiva arqueol√≥gica.
    """
    
    def __init__(self):
        """Inicializar asistente arqueol√≥gico con configuraci√≥n desde .env.local"""
        
        # Leer configuraci√≥n desde .env.local
        self.ollama_enabled = os.getenv('OLLAMA_ENABLED', 'false').lower() == 'true'
        self.openrouter_enabled = os.getenv('OPENROUTER_ENABLED', 'true').lower() == 'true'
        
        # Configuraci√≥n OpenRouter
        self.openrouter_api_key = os.getenv('OPENROUTER_API_KEY')
        self.openrouter_model = os.getenv('OPENROUTER_MODEL', 'google/gemini-2.0-flash-exp:free')
        
        # Configuraci√≥n Ollama (fallback)
        self.ollama_model = os.getenv('OLLAMA_MODEL', 'phi4-mini-reasoning')
        self.ollama_url = os.getenv('OLLAMA_URL', 'http://localhost:11434')
        
        # Timeouts
        self.ai_timeout = int(os.getenv('AI_TIMEOUT_SECONDS', '30'))
        self.max_tokens = int(os.getenv('AI_MAX_TOKENS', '300'))
        
        # Verificar disponibilidad
        self.is_available = self._check_availability()
        
        logger.info(f"ArchaeologicalAssistant inicializado:")
        logger.info(f"  - OpenRouter: {'‚úÖ' if self.openrouter_enabled else '‚ùå'} ({self.openrouter_model})")
        logger.info(f"  - Ollama: {'‚úÖ' if self.ollama_enabled else '‚ùå'} ({self.ollama_model})")
        logger.info(f"  - Disponible: {'‚úÖ' if self.is_available else '‚ùå'}")
        
        # Prompt base arqueol√≥gico
        self.base_prompt = """Eres un especialista en arqueolog√≠a remota y an√°lisis espacial cient√≠fico. 

Tu rol es analizar anomal√≠as detectadas por sensores remotos desde una perspectiva arqueol√≥gica rigurosa.

PRINCIPIOS FUNDAMENTALES:
1. NUNCA afirmes "encontramos una ciudad" o "esto es un templo"
2. SIEMPRE usa lenguaje cient√≠fico: "√°rea con firma espacial consistente con intervenci√≥n humana antigua"
3. Distingue entre: anomal√≠as detectadas vs interpretaciones arqueol√≥gicas
4. Considera procesos naturales alternativos ANTES de sugerir origen antr√≥pico
5. Eval√∫a persistencia temporal y coherencia geom√©trica como indicadores clave

METODOLOG√çA:
- Analiza patrones espaciales no explicables por procesos naturales actuales
- Eval√∫a coherencia entre m√∫ltiples tipos de datos (vegetaci√≥n, t√©rmico, SAR, etc.)
- Considera contexto geogr√°fico y arqueol√≥gico regional
- Proporciona razonamiento paso a paso

FORMATO DE RESPUESTA:
- Explicaci√≥n clara del patr√≥n detectado
- Interpretaci√≥n arqueol√≥gica cautelosa
- Evaluaci√≥n de confianza y limitaciones
- Recomendaciones para investigaci√≥n adicional"""
        
        logger.info(f"ArchaeologicalAssistant inicializado: modelo={self.openrouter_model if self.openrouter_enabled else self.ollama_model}, "
                   f"disponible={self.is_available}")
    
    def _check_availability(self) -> bool:
        """
        Verificar disponibilidad de IA (OpenRouter primero, luego Ollama).
        
        IMPORTANTE: Si falla, el sistema sigue funcionando sin IA.
        La IA es OPCIONAL para explicaciones, no cr√≠tica para detecci√≥n.
        """
        
        # Prioridad 1: OpenRouter
        if self.openrouter_enabled and self.openrouter_api_key:
            try:
                logger.info(f"üîç Verificando OpenRouter con modelo {self.openrouter_model}...")
                
                # Test simple con OpenRouter
                headers = {
                    "Authorization": f"Bearer {self.openrouter_api_key}",
                    "Content-Type": "application/json",
                    "HTTP-Referer": "https://archeoscope.app",
                    "X-Title": "ArcheoScope"
                }
                
                test_payload = {
                    "model": self.openrouter_model,
                    "messages": [{"role": "user", "content": "Test"}],
                    "max_tokens": 10
                }
                
                response = requests.post(
                    "https://openrouter.ai/api/v1/chat/completions",
                    headers=headers,
                    json=test_payload,
                    timeout=10
                )
                
                if response.status_code == 200:
                    logger.info(f"‚úÖ OpenRouter disponible con {self.openrouter_model}")
                    return True
                elif response.status_code == 401:
                    logger.warning(f"‚ö†Ô∏è OpenRouter: API key inv√°lida o expirada")
                elif response.status_code == 404:
                    logger.warning(f"‚ö†Ô∏è OpenRouter: Modelo {self.openrouter_model} no encontrado")
                else:
                    logger.warning(f"‚ö†Ô∏è OpenRouter error: HTTP {response.status_code} - {response.text[:200]}")
                    
            except requests.exceptions.Timeout:
                logger.warning(f"‚ö†Ô∏è OpenRouter: Timeout (red lenta o servicio no responde)")
            except requests.exceptions.ConnectionError:
                logger.warning(f"‚ö†Ô∏è OpenRouter: Error de conexi√≥n (sin internet?)")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Error conectando con OpenRouter: {e}")
        
        # Prioridad 2: Ollama (fallback)
        if self.ollama_enabled:
            try:
                logger.info(f"üîç Verificando Ollama en {self.ollama_url}...")
                response = requests.get(f"{self.ollama_url}/api/tags", timeout=5)
                if response.status_code == 200:
                    models = response.json().get('models', [])
                    available_models = [model['name'] for model in models]
                    
                    # Buscar phi4-mini-reasoning con cualquier tag
                    phi4_models = [m for m in available_models if 'phi4-mini-reasoning' in m]
                    
                    if phi4_models:
                        # Usar el primer modelo phi4 encontrado
                        self.ollama_model = phi4_models[0]
                        logger.info(f"‚úÖ Ollama disponible con {self.ollama_model}")
                        return True
                    else:
                        logger.warning(f"‚ö†Ô∏è Modelo phi4-mini-reasoning no encontrado. "
                                     f"Disponibles: {available_models}")
                        return False
                else:
                    logger.warning(f"‚ö†Ô∏è Ollama no responde: HTTP {response.status_code}")
                    return False
                    
            except requests.exceptions.Timeout:
                logger.warning(f"‚ö†Ô∏è Ollama: Timeout (servicio no responde)")
            except requests.exceptions.ConnectionError:
                logger.warning(f"‚ö†Ô∏è Ollama: No est√° corriendo en {self.ollama_url}")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Error conectando con Ollama: {e}")
        
        logger.error("‚ùå CR√çTICO: Ning√∫n proveedor de IA disponible")
        logger.error("‚ùå El asistente de IA es NECESARIO para an√°lisis arqueol√≥gico riguroso")
        logger.error("‚ùå Por favor verifica:")
        logger.error("   1. OPENROUTER_API_KEY est√° configurada en .env.local")
        logger.error("   2. El modelo est√° disponible en OpenRouter")
        logger.error("   3. Tienes conexi√≥n a internet")
        logger.error("   4. O inicia Ollama con: ollama run phi4-mini-reasoning")
        return False
    
    def explain_archaeological_anomalies(self, 
                                       anomalies: List[Dict[str, Any]], 
                                       rule_evaluations: Dict[str, Any],
                                       context: Dict[str, Any]) -> ArchaeologicalExplanation:
        """
        Explicar anomal√≠as desde perspectiva arqueol√≥gica.
        
        Args:
            anomalies: Lista de anomal√≠as detectadas
            rule_evaluations: Evaluaciones de reglas arqueol√≥gicas
            context: Contexto geogr√°fico y metodol√≥gico
            
        Returns:
            Explicaci√≥n arqueol√≥gica estructurada
        """
        
        if not self.is_available:
            return self._fallback_explanation(anomalies, rule_evaluations, context)
        
        # Construir prompt espec√≠fico
        prompt = self._build_archaeological_prompt(anomalies, rule_evaluations, context)
        
        try:
            # Llamar al modelo
            response = self._call_ai_model(prompt)
            
            # Parsear respuesta
            return self._parse_archaeological_response(response, anomalies, context)
            
        except Exception as e:
            logger.error(f"Error en explicaci√≥n arqueol√≥gica: {e}")
            return self._fallback_explanation(anomalies, rule_evaluations, context)
    
    def _build_archaeological_prompt(self, 
                                   anomalies: List[Dict[str, Any]], 
                                   rule_evaluations: Dict[str, Any],
                                   context: Dict[str, Any]) -> str:
        """Construir prompt arqueol√≥gico espec√≠fico."""
        
        # Informaci√≥n del contexto
        region_info = f"""
CONTEXTO REGIONAL:
- Regi√≥n: {context.get('region_name', 'Desconocida')}
- √Årea: {context.get('area_km2', 0):,.0f} km¬≤
- Coordenadas: {context.get('coordinates', 'No especificadas')}
- Tipo de paisaje: {context.get('landscape_type', 'Mixto')}
"""
        
        # Resumen de anomal√≠as
        anomaly_summary = "ANOMAL√çAS DETECTADAS:\n"
        for i, anomaly in enumerate(anomalies, 1):
            anomaly_summary += f"""
{i}. Tipo: {anomaly.get('type', 'Desconocido')}
   - Probabilidad arqueol√≥gica: {anomaly.get('archaeological_probability', 0):.2f}
   - Coherencia geom√©trica: {anomaly.get('geometric_coherence', 0):.2f}
   - Persistencia temporal: {anomaly.get('temporal_persistence', 0):.2f}
   - P√≠xeles afectados: {anomaly.get('affected_pixels', 0):,}
   - Caracter√≠sticas: {', '.join(anomaly.get('suspected_features', []))}
"""
        
        # Evaluaciones de reglas
        rules_summary = "EVALUACIONES DE REGLAS:\n"
        for rule_name, evaluation in rule_evaluations.items():
            if hasattr(evaluation, 'result'):
                rules_summary += f"""
- {rule_name}: {evaluation.result.value}
  Probabilidad arqueol√≥gica: {evaluation.archaeological_probability:.2f}
  Violaciones: {', '.join(evaluation.rule_violations) if evaluation.rule_violations else 'Ninguna'}
"""
        
        # Prompt completo
        full_prompt = f"""{self.base_prompt}

{region_info}

{anomaly_summary}

{rules_summary}

TAREA:
Analiza estos hallazgos desde una perspectiva arqueol√≥gica cient√≠fica. Proporciona:

1. EXPLICACI√ìN CLARA: ¬øQu√© patrones espaciales se detectaron?
2. INTERPRETACI√ìN ARQUEOL√ìGICA: ¬øQu√© podr√≠an indicar estos patrones? (cauteloso)
3. RAZONAMIENTO CIENT√çFICO: ¬øPor qu√© estos patrones son significativos?
4. EVALUACI√ìN DE CONFIANZA: ¬øQu√© tan confiables son estas interpretaciones?
5. LIMITACIONES: ¬øQu√© no podemos concluir con certeza?
6. RECOMENDACIONES: ¬øQu√© investigaci√≥n adicional se necesita?

Recuerda: Nunca afirmes descubrimientos definitivos. Usa lenguaje cient√≠fico cauteloso."""
        
        return full_prompt
    
    def _call_ai_model(self, prompt: str) -> str:
        """Llamar al modelo de IA (OpenRouter primero, luego Ollama)."""
        
        # Prioridad 1: OpenRouter
        if self.openrouter_enabled and self.openrouter_api_key:
            try:
                headers = {
                    "Authorization": f"Bearer {self.openrouter_api_key}",
                    "Content-Type": "application/json"
                }
                
                payload = {
                    "model": self.openrouter_model,
                    "messages": [
                        {
                            "role": "system", 
                            "content": "Eres un arque√≥logo experto especializado en teledetecci√≥n arqueol√≥gica. Proporciona an√°lisis cient√≠ficos rigurosos y explicaciones claras sobre anomal√≠as espaciales."
                        },
                        {
                            "role": "user", 
                            "content": prompt
                        }
                    ],
                    "max_tokens": self.max_tokens,
                    "temperature": 0.3,  # M√°s determinista para an√°lisis cient√≠fico
                    "top_p": 0.9
                }
                
                response = requests.post(
                    "https://openrouter.ai/api/v1/chat/completions",
                    headers=headers,
                    json=payload,
                    timeout=self.ai_timeout
                )
                
                if response.status_code == 200:
                    result = response.json()
                    content = result['choices'][0]['message']['content']
                    logger.info(f"‚úÖ Respuesta de OpenRouter ({self.openrouter_model})")
                    return content
                else:
                    logger.warning(f"OpenRouter error: {response.status_code}")
                    
            except Exception as e:
                logger.warning(f"Error con OpenRouter: {e}")
        
        # Prioridad 2: Ollama (fallback)
        if self.ollama_enabled:
            try:
                payload = {
                    "model": self.ollama_model,
                    "prompt": prompt,
                    "stream": False,
                    "options": {
                        "temperature": 0.3,
                        "top_p": 0.9,
                        "num_predict": 1000
                    }
                }
                
                response = requests.post(
                    f"{self.ollama_url}/api/generate",
                    json=payload,
                    timeout=60
                )
                
                if response.status_code == 200:
                    result = response.json()
                    logger.info(f"‚úÖ Respuesta de Ollama ({self.ollama_model})")
                    return result.get('response', '')
                else:
                    raise Exception(f"Ollama error: {response.status_code} - {response.text}")
                    
            except Exception as e:
                logger.warning(f"Error con Ollama: {e}")
        
        raise Exception("Ning√∫n proveedor de IA disponible")
    
    def _parse_archaeological_response(self, 
                                     response: str, 
                                     anomalies: List[Dict[str, Any]], 
                                     context: Dict[str, Any]) -> ArchaeologicalExplanation:
        """Parsear respuesta del modelo en estructura arqueol√≥gica."""
        
        # Extraer secciones de la respuesta
        sections = self._extract_response_sections(response)
        
        # Generar recomendaciones espec√≠ficas
        recommendations = self._generate_archaeological_recommendations(anomalies, context)
        
        return ArchaeologicalExplanation(
            explanation=sections.get('explanation', response[:500] + "..."),
            archaeological_interpretation=sections.get('interpretation', 
                "An√°lisis requiere datos adicionales para interpretaci√≥n arqueol√≥gica definitiva."),
            confidence_assessment=sections.get('confidence', 
                "Confianza moderada - requiere validaci√≥n con m√©todos adicionales."),
            methodological_notes=sections.get('methodology', 
                "An√°lisis basado en detecci√≥n de anomal√≠as espaciales multiespectrales."),
            recommendations=recommendations,
            limitations=sections.get('limitations', 
                "Interpretaci√≥n basada en sensores remotos - requiere validaci√≥n de campo."),
            scientific_reasoning=sections.get('reasoning', 
                "Patrones detectados muestran persistencia espacial y coherencia geom√©trica.")
        )
    
    def _extract_response_sections(self, response: str) -> Dict[str, str]:
        """Extraer secciones estructuradas de la respuesta."""
        
        sections = {}
        current_section = 'explanation'
        current_text = []
        
        lines = response.split('\n')
        
        for line in lines:
            line = line.strip()
            
            # Detectar nuevas secciones
            if any(keyword in line.lower() for keyword in ['interpretaci√≥n', 'interpretation']):
                sections[current_section] = '\n'.join(current_text)
                current_section = 'interpretation'
                current_text = []
            elif any(keyword in line.lower() for keyword in ['confianza', 'confidence']):
                sections[current_section] = '\n'.join(current_text)
                current_section = 'confidence'
                current_text = []
            elif any(keyword in line.lower() for keyword in ['limitaciones', 'limitations']):
                sections[current_section] = '\n'.join(current_text)
                current_section = 'limitations'
                current_text = []
            elif any(keyword in line.lower() for keyword in ['razonamiento', 'reasoning']):
                sections[current_section] = '\n'.join(current_text)
                current_section = 'reasoning'
                current_text = []
            else:
                if line:  # Solo a√±adir l√≠neas no vac√≠as
                    current_text.append(line)
        
        # A√±adir √∫ltima secci√≥n
        sections[current_section] = '\n'.join(current_text)
        
        return sections
    
    def _generate_archaeological_recommendations(self, 
                                              anomalies: List[Dict[str, Any]], 
                                              context: Dict[str, Any]) -> List[str]:
        """Generar recomendaciones arqueol√≥gicas espec√≠ficas."""
        
        recommendations = []
        
        # Recomendaciones basadas en tipos de anomal√≠as
        has_vegetation_anomalies = any('vegetation' in str(a.get('type', '')) for a in anomalies)
        has_thermal_anomalies = any('thermal' in str(a.get('type', '')) for a in anomalies)
        has_geometric_patterns = any(a.get('geometric_coherence', 0) > 0.5 for a in anomalies)
        
        if has_vegetation_anomalies:
            recommendations.append(
                "An√°lisis multitemporal de √≠ndices de vegetaci√≥n para confirmar persistencia de patrones"
            )
        
        if has_thermal_anomalies:
            recommendations.append(
                "Adquisici√≥n de datos t√©rmicos nocturnos adicionales para validar inercia t√©rmica diferencial"
            )
        
        if has_geometric_patterns:
            recommendations.append(
                "Investigaci√≥n geof√≠sica (GPR, magnetometr√≠a) para validar estructuras subsuperficiales"
            )
        
        # Recomendaciones generales
        recommendations.extend([
            "An√°lisis de contexto arqueol√≥gico regional para evaluar plausibilidad cultural",
            "Adquisici√≥n de datos de mayor resoluci√≥n espacial para an√°lisis detallado",
            "Correlaci√≥n con bases de datos arqueol√≥gicas existentes"
        ])
        
        return recommendations
    
    def _fallback_explanation(self, 
                            anomalies: List[Dict[str, Any]], 
                            rule_evaluations: Dict[str, Any],
                            context: Dict[str, Any]) -> ArchaeologicalExplanation:
        """Generar explicaci√≥n de respaldo cuando IA no est√° disponible."""
        
        # An√°lisis determinista b√°sico
        total_anomalies = len(anomalies)
        high_prob_anomalies = sum(1 for a in anomalies if a.get('archaeological_probability', 0) > 0.6)
        
        if high_prob_anomalies > 0:
            explanation = (f"Se detectaron {total_anomalies} anomal√≠as espaciales, "
                         f"de las cuales {high_prob_anomalies} muestran alta probabilidad "
                         f"de origen antr√≥pico basado en criterios de persistencia espacial "
                         f"y coherencia geom√©trica.")
            
            interpretation = ("Las anomal√≠as detectadas muestran patrones no explicables "
                            "completamente por procesos naturales actuales. La coherencia "
                            "geom√©trica y persistencia espacial sugieren posible intervenci√≥n "
                            "humana antigua, aunque se requiere validaci√≥n adicional.")
            
            confidence = "Confianza moderada basada en an√°lisis determinista"
        else:
            explanation = (f"Se detectaron {total_anomalies} anomal√≠as espaciales menores. "
                         f"Los patrones observados son consistentes con variabilidad natural "
                         f"o procesos geol√≥gicos/ecol√≥gicos actuales.")
            
            interpretation = ("Las anomal√≠as detectadas no muestran caracter√≠sticas "
                            "distintivas de intervenci√≥n humana antigua. Los patrones "
                            "son explicables por procesos naturales conocidos.")
            
            confidence = "Alta confianza en explicaci√≥n por procesos naturales"
        
        return ArchaeologicalExplanation(
            explanation=explanation,
            archaeological_interpretation=interpretation,
            confidence_assessment=confidence,
            methodological_notes="An√°lisis determinista - IA no disponible",
            recommendations=[
                "An√°lisis con datos de mayor resoluci√≥n",
                "Validaci√≥n con m√©todos geof√≠sicos",
                "Consulta con arque√≥logos regionales"
            ],
            limitations="An√°lisis limitado por ausencia de IA especializada",
            scientific_reasoning="Basado en criterios de persistencia espacial y coherencia geom√©trica"
        )
    
    def explain_batch_archaeological_analysis(self, 
                                            spatial_anomalies: List[Dict[str, Any]], 
                                            rule_contradictions: List[Dict[str, Any]], 
                                            context: Dict[str, Any]) -> ArchaeologicalExplanation:
        """
        Explicar an√°lisis arqueol√≥gico por lotes.
        
        Args:
            spatial_anomalies: Anomal√≠as espaciales detectadas
            rule_contradictions: Contradicciones de reglas arqueol√≥gicas
            context: Contexto del an√°lisis
            
        Returns:
            Explicaci√≥n arqueol√≥gica integrada
        """
        
        # Combinar anomal√≠as y contradicciones
        all_anomalies = spatial_anomalies + [
            {
                'type': f"rule_violation_{c.get('rule', 'unknown')}",
                'archaeological_probability': c.get('archaeological_probability', 0.5),
                'geometric_coherence': c.get('geometric_coherence', 0.0),
                'temporal_persistence': c.get('temporal_persistence', 0.0),
                'affected_pixels': c.get('pixels', 0),
                'suspected_features': [c.get('rule', 'unknown_violation')]
            }
            for c in rule_contradictions
        ]
        
        # Crear evaluaciones simuladas para compatibilidad
        mock_evaluations = {
            f"rule_{i}": type('MockEval', (), {
                'result': type('MockResult', (), {'value': 'anomalous'})(),
                'archaeological_probability': anomaly.get('archaeological_probability', 0.5),
                'rule_violations': anomaly.get('suspected_features', [])
            })()
            for i, anomaly in enumerate(all_anomalies)
        }
        
        return self.explain_archaeological_anomalies(all_anomalies, mock_evaluations, context)