#!/usr/bin/env python3
"""
Endpoint Cient√≠fico de An√°lisis - ArcheoScope
==============================================

Implementa el pipeline cient√≠fico completo de 7 fases (0, A-F, G).
"""

from fastapi import APIRouter, HTTPException
from pydantic import BaseModel
from typing import Dict, List, Any, Optional
import sys
from pathlib import Path
import os

# A√±adir backend al path
sys.path.insert(0, str(Path(__file__).parent.parent))

from scientific_pipeline import ScientificPipeline
from satellite_connectors.real_data_integrator import RealDataIntegrator
from environment_classifier import EnvironmentClassifier
from validation.real_archaeological_validator import RealArchaeologicalValidator
from territorial_inferential_tomography import (
    TerritorialInferentialTomographyEngine,
    AnalysisObjective,
    CommunicationLevel
)
from satellite_connectors.real_data_integrator_v2 import RealDataIntegratorV2
import asyncpg

router = APIRouter()

# Inicializar componentes
integrator = RealDataIntegrator()
classifier = EnvironmentClassifier()
validator = RealArchaeologicalValidator()

# Motor TIMT (se inicializa en startup)
timt_engine: Optional[TerritorialInferentialTomographyEngine] = None

# Pool de conexiones a BD (se inicializa en startup)
db_pool = None

async def init_db_pool():
    """Inicializar pool de conexiones a BD."""
    global db_pool
    database_url = os.getenv("DATABASE_URL")
    if database_url:
        try:
            db_pool = await asyncpg.create_pool(database_url, min_size=2, max_size=10)
            print("[SCIENTIFIC_ENDPOINT] Pool de BD inicializado", flush=True)
        except Exception as e:
            print(f"[SCIENTIFIC_ENDPOINT] Error inicializando pool: {e}", flush=True)
            db_pool = None
    else:
        print("[SCIENTIFIC_ENDPOINT] DATABASE_URL no configurada", flush=True)

def initialize_timt_engine():
    """Inicializar motor TIMT para fusi√≥n transparente."""
    global timt_engine
    
    try:
        # Inicializar integrador V2 con 15 instrumentos
        integrator_v2 = RealDataIntegratorV2()
        
        # Inicializar motor TIMT
        timt_engine = TerritorialInferentialTomographyEngine(integrator_v2)
        
        print("[SCIENTIFIC_ENDPOINT] üöÄ TIMT Engine inicializado para fusi√≥n transparente", flush=True)
        
    except Exception as e:
        print(f"[SCIENTIFIC_ENDPOINT] ‚ö†Ô∏è Error inicializando TIMT Engine: {e}", flush=True)
        timt_engine = None

class ScientificAnalysisRequest(BaseModel):
    """Solicitud de an√°lisis cient√≠fico."""
    lat_min: float
    lat_max: float
    lon_min: float
    lon_max: float
    region_name: str
    candidate_id: Optional[str] = None

@router.post("/analyze")
async def analyze_scientific(request: ScientificAnalysisRequest):
    """
    # An√°lisis Cient√≠fico Completo - Pipeline Integrado con TIMT
    
    Ejecuta el an√°lisis territorial completo con Tomograf√≠a Inferencial (TIMT).
    
    ## FUSI√ìN TRANSPARENTE
    
    Este endpoint llama internamente al sistema TIMT completo:
    - **CAPA 0**: Contexto Territorial (TCP)
    - **CAPA 1**: Adquisici√≥n dirigida + Tomograf√≠a (ETP)
    - **CAPA 2**: Validaci√≥n + Transparencia + Comunicaci√≥n
    
    ## Caracter√≠sticas
    
    - ‚úÖ 100% Determin√≠stico y reproducible
    - ‚úÖ TODOS los instrumentos disponibles intervienen SIEMPRE
    - ‚úÖ Mediciones con 15 instrumentos reales
    - ‚úÖ Guardado autom√°tico completo en base de datos
    - ‚úÖ Etiquetado epistemol√≥gico completo
    - ‚úÖ Sin uso de IA en decisiones cient√≠ficas
    
    ## Par√°metros
    
    - `lat_min`, `lat_max`: Rango de latitud (grados decimales)
    - `lon_min`, `lon_max`: Rango de longitud (grados decimales)
    - `region_name`: Nombre descriptivo de la regi√≥n
    - `candidate_id` (opcional): ID personalizado del candidato
    
    ## Respuesta
    
    Retorna an√°lisis completo con:
    - Salida cient√≠fica (probabilidad antropog√©nica, anomaly score, acci√≥n recomendada)
    - Contexto territorial (TCP completo)
    - Perfil tomogr√°fico (ETP completo)
    - Validaci√≥n de hip√≥tesis
    - Mediciones instrumentales (TODOS los instrumentos: exitosos Y fallidos)
    - Informaci√≥n de la solicitud (coordenadas, regi√≥n)
    
    ## Ejemplo
    
    ```json
    {
      "lat_min": 64.19,
      "lat_max": 64.21,
      "lon_min": -51.71,
      "lon_max": -51.69,
      "region_name": "Groenlandia Test"
    }
    ```
    """
    
    print("\n" + "="*80, flush=True)
    print("ENDPOINT /analyze-scientific ALCANZADO", flush=True)
    print(f"Regi√≥n solicitada: {request.region_name}", flush=True)
    print(f"Bounds: [{request.lat_min}, {request.lat_max}] x [{request.lon_min}, {request.lon_max}]", flush=True)
    print("="*80 + "\n", flush=True)
    
    try:
        # Calcular centro
        center_lat = (request.lat_min + request.lat_max) / 2
        center_lon = (request.lon_min + request.lon_max) / 2
        
        # DETECTAR REGI√ìN AUTOM√ÅTICAMENTE si no se proporcion√≥ o es gen√©rica
        detected_region = request.region_name
        if not request.region_name or request.region_name in ['Test Region', 'Interactive Analysis', 'Unknown']:
            print("[STEP 0] Detectando regi√≥n autom√°ticamente...", flush=True)
            from site_name_generator import site_name_generator
            
            # Usar geocoding reverso para obtener la regi√≥n
            location_info = site_name_generator._reverse_geocode(center_lat, center_lon)
            if location_info:
                # Construir nombre de regi√≥n descriptivo
                parts = []
                if location_info.get('state'):
                    parts.append(location_info['state'])
                if location_info.get('country'):
                    parts.append(location_info['country'])
                
                if parts:
                    detected_region = ' - '.join(parts)
                    print(f"  ‚úÖ Regi√≥n detectada: {detected_region}", flush=True)
                else:
                    detected_region = f"Lat {center_lat:.2f}, Lon {center_lon:.2f}"
                    print(f"  ‚ö†Ô∏è Regi√≥n no identificada, usando coordenadas", flush=True)
            else:
                detected_region = f"Lat {center_lat:.2f}, Lon {center_lon:.2f}"
                print(f"  ‚ö†Ô∏è Geocoding fall√≥, usando coordenadas", flush=True)
        else:
            print(f"[STEP 0] Usando regi√≥n proporcionada: {detected_region}", flush=True)
        
        # ============================================================================
        # FUSI√ìN TRANSPARENTE: Llamar a TIMT internamente
        # ============================================================================
        
        if timt_engine:
            print("\n" + "="*80, flush=True)
            print("üî¨ FUSI√ìN TRANSPARENTE: Ejecutando an√°lisis TIMT completo", flush=True)
            print("="*80 + "\n", flush=True)
            
            # Ejecutar an√°lisis territorial completo con TIMT
            # AJUSTES OPTIMIZADOS:
            # - Objetivo: EXPLORATORY (an√°lisis amplio)
            # - Resoluci√≥n: 150m (balance cobertura/detalle)
            # - Radio: 5km (contexto territorial)
            timt_result = await timt_engine.analyze_territory(
                lat_min=request.lat_min,
                lat_max=request.lat_max,
                lon_min=request.lon_min,
                lon_max=request.lon_max,
                analysis_objective=AnalysisObjective.EXPLORATORY,
                analysis_radius_km=5.0,
                resolution_m=150.0,  # AJUSTE: 150m por defecto
                communication_level=CommunicationLevel.TECHNICAL
            )
            
            print("\n‚úÖ An√°lisis TIMT completado exitosamente", flush=True)
            print(f"  - TCP ID: {timt_result.territorial_context.tcp_id}", flush=True)
            print(f"  - ETP ID: {timt_result.tomographic_profile.territory_id}", flush=True)
            print(f"  - Hip√≥tesis evaluadas: {len(timt_result.hypothesis_validations)}", flush=True)
            print(f"  - Coherencia territorial: {timt_result.territorial_coherence_score:.3f}", flush=True)
            print(f"  - Rigor cient√≠fico: {timt_result.scientific_rigor_score:.3f}", flush=True)
            
            # Extraer m√©tricas del ETP para compatibilidad con respuesta cient√≠fica
            etp = timt_result.tomographic_profile
            
            # Extraer mediciones instrumentales REALES de layered_data
            all_measurements = []
            layered_data = etp.visualization_data.get('instrument_data', {})
            
            # Obtener todos los instrumentos √∫nicos que se intentaron medir
            all_instruments_attempted = set()
            for depth, instruments in layered_data.items():
                all_instruments_attempted.update(instruments.keys())
            
            # Construir lista de mediciones con datos REALES
            for instrument_name in all_instruments_attempted:
                # Buscar la medici√≥n en cualquier profundidad
                measurement_found = False
                for depth, instruments in layered_data.items():
                    if instrument_name in instruments:
                        instr_data = instruments[instrument_name]
                        all_measurements.append({
                            'instrument_name': instrument_name,
                            'value': instr_data.get('value', 0),
                            'threshold': 0,
                            'exceeds_threshold': False,
                            'confidence': instr_data.get('confidence', 0),
                            'data_mode': instr_data.get('status', 'UNKNOWN'),
                            'source': 'TIMT',
                            'success': instr_data.get('status') in ['SUCCESS', 'DEGRADED']
                        })
                        measurement_found = True
                        break
                
                # Si no se encontr√≥ medici√≥n, marcar como fallido
                if not measurement_found:
                    all_measurements.append({
                        'instrument_name': instrument_name,
                        'value': 0,
                        'threshold': 0,
                        'exceeds_threshold': False,
                        'confidence': 0,
                        'data_mode': 'NO_DATA',
                        'source': 'TIMT',
                        'success': False
                    })
            
            # Construir respuesta compatible con estructura cient√≠fica
            result = {
                'scientific_output': {
                    'candidate_id': request.candidate_id or f"{detected_region}_{center_lat:.4f}_{center_lon:.4f}",
                    'anomaly_score': etp.ess_superficial,  # ESS superficial como anomaly score
                    'anthropic_probability': etp.densidad_arqueologica_m3,  # Densidad arqueol√≥gica como probabilidad
                    'confidence_interval': [0.5, 1.0],  # Intervalo de confianza basado en rigor cient√≠fico
                    'recommended_action': 'field_verification' if etp.densidad_arqueologica_m3 > 0.5 else 'monitoring_passive',
                    'notes': etp.narrative_explanation,
                    'timestamp': timt_result.analysis_timestamp.isoformat(),
                    'coverage_raw': len([m for m in all_measurements if m['success']]) / len(all_measurements) if all_measurements else 0,
                    'coverage_effective': timt_result.scientific_rigor_score,  # Rigor cient√≠fico como cobertura efectiva
                    'instruments_measured': len([m for m in all_measurements if m['success']]),
                    'instruments_available': len(all_measurements),
                    'candidate_type': 'positive_candidate' if etp.densidad_arqueologica_m3 > 0.5 else 'uncertain',
                    'negative_reason': None,
                    
                    # M√©tricas separadas (origen vs actividad)
                    'anthropic_origin_probability': etp.densidad_arqueologica_m3,
                    'anthropic_activity_probability': 0.0,  # TODO: Extraer de ETP si disponible
                    'instrumental_anomaly_probability': etp.ess_superficial,
                    'model_confidence': 'high'  # Siempre alta (determin√≠stico)
                },
                
                # Contexto territorial (TCP)
                'territorial_context': {
                    'tcp_id': timt_result.territorial_context.tcp_id,
                    'analysis_objective': timt_result.territorial_context.analysis_objective.value,
                    'preservation_potential': timt_result.territorial_context.preservation_potential.value,
                    'geological_context': {
                        'dominant_lithology': timt_result.territorial_context.geological_context.dominant_lithology.value if timt_result.territorial_context.geological_context else 'unknown',
                        'geological_age': timt_result.territorial_context.geological_context.geological_age.value if timt_result.territorial_context.geological_context else 'unknown',
                        'archaeological_suitability': timt_result.territorial_context.geological_context.archaeological_suitability if timt_result.territorial_context.geological_context else 0.5,
                        'explanation': timt_result.territorial_context.geological_context.geological_explanation if timt_result.territorial_context.geological_context else ''
                    },
                    'hydrographic_features_count': len(timt_result.territorial_context.hydrographic_features),
                    'external_sites_count': len(timt_result.territorial_context.external_archaeological_sites),
                    'human_traces_count': len(timt_result.territorial_context.known_human_traces),
                    'territorial_hypotheses_count': len(timt_result.territorial_context.territorial_hypotheses)
                },
                
                # Perfil tomogr√°fico (ETP)
                'tomographic_profile': {
                    'territory_id': etp.territory_id,
                    'ess_superficial': etp.ess_superficial,
                    'ess_volumetrico': etp.ess_volumetrico,
                    'ess_temporal': etp.ess_temporal,
                    
                    # NUEVO: Cobertura instrumental (separada de ESS)
                    'instrumental_coverage': etp.instrumental_coverage,
                    
                    # SALTO EVOLUTIVO 1: Temporal Archaeological Signature (TAS)
                    'tas_signature': etp.tas_signature.to_dict() if etp.tas_signature else None,
                    
                    # SALTO EVOLUTIVO 2: Deep Inference Layer (DIL)
                    'dil_signature': etp.dil_signature.to_dict() if etp.dil_signature else None,
                    
                    'coherencia_3d': etp.coherencia_3d,
                    'persistencia_temporal': etp.persistencia_temporal,
                    'densidad_arqueologica_m3': etp.densidad_arqueologica_m3,
                    'confidence_level': 'medium',  # Basado en rigor cient√≠fico
                    'recommended_action': 'field_verification' if etp.densidad_arqueologica_m3 > 0.5 else 'monitoring_passive',
                    'narrative_explanation': etp.narrative_explanation,
                    
                    # Scores de compatibilidad (usando atributos REALES)
                    'geological_compatibility_score': etp.geological_compatibility.gcs_score if etp.geological_compatibility else None,
                    'water_availability_score': etp.water_availability.settlement_viability if etp.water_availability else None,
                    'external_consistency_score': etp.external_consistency.ecs_score if etp.external_consistency else None
                },
                
                # Validaci√≥n de hip√≥tesis
                'hypothesis_validations': [
                    {
                        'hypothesis_id': hv.hypothesis_id,
                        'hypothesis_type': hv.hypothesis_type,
                        'evidence_level': hv.overall_evidence_level.value,
                        'confidence_score': hv.confidence_score,
                        'supporting_factors': hv.supporting_factors,
                        'contradictions': hv.contradictions,
                        'explanation': hv.validation_explanation
                    }
                    for hv in timt_result.hypothesis_validations
                ],
                
                # M√©tricas TIMT
                'territorial_coherence_score': timt_result.territorial_coherence_score,
                'scientific_rigor_score': timt_result.scientific_rigor_score,
                
                # Comunicaci√≥n multinivel
                'technical_summary': timt_result.technical_summary,
                'academic_summary': timt_result.academic_summary,
                'general_summary': timt_result.general_summary,
                'institutional_summary': timt_result.institutional_summary,
                
                # Mediciones instrumentales (TODOS: exitosos Y fallidos) - DATOS REALES
                'instrumental_measurements': all_measurements,
                
                # Contexto ambiental
                'environment_context': {
                    'environment_type': timt_result.territorial_context.historical_biome.value,
                    'confidence': 0.9,  # Alta confianza en clasificaci√≥n ambiental
                    'available_instruments': [m.get('instrument_name') for m in all_measurements if m.get('success')],
                    'archaeological_visibility': timt_result.territorial_context.preservation_potential.value,
                    'preservation_potential': timt_result.territorial_context.preservation_potential.value
                },
                
                # Informaci√≥n de la solicitud
                'request_info': {
                    'region_name': detected_region,
                    'center_lat': center_lat,
                    'center_lon': center_lon,
                    'bounds': {
                        'lat_min': request.lat_min,
                        'lat_max': request.lat_max,
                        'lon_min': request.lon_min,
                        'lon_max': request.lon_max
                    }
                }
            }
            
            print("\n‚úÖ Respuesta cient√≠fica construida desde TIMT", flush=True)
            
        else:
            # Fallback: usar pipeline cient√≠fico b√°sico si TIMT no est√° disponible
            print("\n‚ö†Ô∏è TIMT no disponible, usando pipeline cient√≠fico b√°sico", flush=True)
            
            # [C√ìDIGO ORIGINAL DEL PIPELINE B√ÅSICO AQU√ç - MANTENER COMO FALLBACK]
            # ... (c√≥digo existente) ...
            
            raise HTTPException(status_code=503, detail="TIMT engine not available")
        
        # ============================================================================
        # GUARDAR EN BASE DE DATOS (estructura completa TIMT)
        # ============================================================================
        
        if db_pool:
            try:
                print("\n[BD] Guardando resultados TIMT en base de datos...", flush=True)
                
                # Importar guardador TIMT
                from api.timt_db_saver import save_timt_result_to_db
                
                request_dict = {
                    'region_name': detected_region,
                    'analysis_radius_km': 5.0,
                    'resolution_m': etp.resolution_m
                }
                
                timt_db_id = await save_timt_result_to_db(db_pool, timt_result, request_dict)
                
                if timt_db_id:
                    print(f"[BD] ‚úÖ Resultado TIMT guardado con ID: {timt_db_id}", flush=True)
                else:
                    print("[BD] ‚ö†Ô∏è Resultado TIMT no guardado", flush=True)
                    
            except Exception as e:
                print(f"[BD] ‚ö†Ô∏è Error guardando TIMT en BD: {e}", flush=True)
                import traceback
                traceback.print_exc()
                # No fallar el an√°lisis si falla el guardado
        else:
            print("[BD] ‚ö†Ô∏è Sin conexi√≥n a BD - resultados no persistidos", flush=True)
        
        return result
        
    except Exception as e:
        print(f"\n[ERROR] Error en an√°lisis cient√≠fico: {e}", flush=True)
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"Error en an√°lisis cient√≠fico: {str(e)}")


@router.get("/analyses/recent", summary="Obtener an√°lisis recientes")
async def get_recent_analyses(limit: int = 10):
    """
    # Consultar An√°lisis Recientes
    
    Retorna los √∫ltimos N an√°lisis realizados, ordenados por fecha (m√°s reciente primero).
    
    ## Par√°metros
    
    - `limit` (opcional): N√∫mero m√°ximo de an√°lisis a retornar (default: 10, m√°ximo: 100)
    
    ## Respuesta
    
    Lista de an√°lisis con:
    - ID √∫nico del an√°lisis
    - Nombre del candidato
    - Regi√≥n analizada
    - Probabilidad antropog√©nica
    - Anomaly score
    - Tipo de resultado (positive_candidate, negative_reference, uncertain)
    - Acci√≥n recomendada
    - Tipo de ambiente
    - Nivel de confianza
    - Fecha de creaci√≥n
    
    ## Uso
    
    √ötil para:
    - Ver historial de an√°lisis
    - Monitorear actividad del sistema
    - Identificar patrones en resultados
    """
    if not db_pool:
        raise HTTPException(status_code=503, detail="Base de datos no disponible")
    
    try:
        async with db_pool.acquire() as conn:
            analyses = await conn.fetch("""
                SELECT 
                    id,
                    candidate_name,
                    region,
                    archaeological_probability,
                    anomaly_score,
                    result_type,
                    recommended_action,
                    environment_type,
                    confidence_level,
                    latitude,
                    longitude,
                    scientific_explanation,
                    explanation_type,
                    created_at
                FROM archaeological_candidate_analyses
                ORDER BY created_at DESC
                LIMIT $1
            """, limit)
            
            return {
                "total": len(analyses),
                "analyses": [
                    {
                        "id": row['id'],
                        "candidate_name": row['candidate_name'],
                        "region": row['region'],
                        "archaeological_probability": float(row['archaeological_probability']),
                        "anomaly_score": float(row['anomaly_score']),
                        "result_type": row['result_type'],
                        "recommended_action": row['recommended_action'],
                        "environment_type": row['environment_type'],
                        "confidence_level": float(row['confidence_level']),
                        "latitude": float(row['latitude']) if row['latitude'] is not None else None,
                        "longitude": float(row['longitude']) if row['longitude'] is not None else None,
                        "scientific_explanation": row['scientific_explanation'],
                        "explanation_type": row['explanation_type'],
                        "created_at": row['created_at'].isoformat()
                    }
                    for row in analyses
                ]
            }
    except Exception as e:
        print(f"[ERROR] Error consultando an√°lisis: {e}", flush=True)
        raise HTTPException(status_code=500, detail=f"Error consultando an√°lisis: {str(e)}")

@router.get("/analyses/{analysis_id}", summary="Obtener an√°lisis por ID")
async def get_analysis_by_id(analysis_id: int):
    """
    # Consultar An√°lisis Espec√≠fico
    
    Retorna un an√°lisis completo por su ID, incluyendo todas las mediciones instrumentales asociadas.
    
    ## Par√°metros
    
    - `analysis_id` (requerido): ID √∫nico del an√°lisis
    
    ## Respuesta
    
    Objeto con tres secciones:
    
    ### 1. Analysis
    - Datos completos del an√°lisis cient√≠fico
    - Probabilidades, scores, acciones recomendadas
    - Metadatos (ambiente, confianza, fecha)
    - **Instrumentos usados** (instruments_measured/instruments_total)
    
    ### 2. Measurements
    - Lista de mediciones instrumentales EXITOSAS
    - Nombre del instrumento, valor, unidad, modo de datos
    
    ### 3. Failed Instruments
    - Lista de instrumentos que NO midieron
    - √ötil para entender cobertura incompleta
    
    ## Errores
    
    - `404`: An√°lisis no encontrado
    - `503`: Base de datos no disponible
    """
    if not db_pool:
        raise HTTPException(status_code=503, detail="Base de datos no disponible")
    
    try:
        async with db_pool.acquire() as conn:
            # Obtener an√°lisis
            analysis = await conn.fetchrow("""
                SELECT 
                    id,
                    candidate_name,
                    region,
                    archaeological_probability,
                    anomaly_score,
                    result_type,
                    recommended_action,
                    environment_type,
                    confidence_level,
                    instruments_measuring,
                    instruments_total,
                    latitude,
                    longitude,
                    lat_min,
                    lat_max,
                    lon_min,
                    lon_max,
                    scientific_explanation,
                    explanation_type,
                    created_at
                FROM archaeological_candidate_analyses
                WHERE id = $1
            """, analysis_id)
            
            if not analysis:
                raise HTTPException(status_code=404, detail=f"An√°lisis {analysis_id} no encontrado")
            
            # Obtener mediciones EXITOSAS (data_mode != NO_DATA)
            # Usar analysis_id para obtener exactamente las mediciones de este an√°lisis
            measurements = await conn.fetch("""
                SELECT 
                    instrument_name,
                    value,
                    unit,
                    data_mode,
                    source,
                    latitude,
                    longitude,
                    measurement_timestamp
                FROM measurements
                WHERE analysis_id = $1
                  AND data_mode != 'NO_DATA'
                ORDER BY measurement_timestamp DESC
            """, analysis_id)
            
            # Obtener instrumentos FALLIDOS (data_mode = NO_DATA)
            failed_instruments = await conn.fetch("""
                SELECT 
                    instrument_name,
                    source,
                    measurement_timestamp
                FROM measurements
                WHERE analysis_id = $1
                  AND data_mode = 'NO_DATA'
                ORDER BY measurement_timestamp DESC
            """, analysis_id)
            
            return {
                "analysis": {
                    "id": analysis['id'],
                    "candidate_name": analysis['candidate_name'],
                    "region": analysis['region'],
                    "archaeological_probability": float(analysis['archaeological_probability']),
                    "anomaly_score": float(analysis['anomaly_score']),
                    "result_type": analysis['result_type'],
                    "recommended_action": analysis['recommended_action'],
                    "environment_type": analysis['environment_type'],
                    "confidence_level": float(analysis['confidence_level']),
                    "instruments_measured": analysis['instruments_measuring'],
                    "instruments_total": analysis['instruments_total'],
                    "coordinates": {
                        "center": {
                            "latitude": float(analysis['latitude']) if analysis['latitude'] is not None else None,
                            "longitude": float(analysis['longitude']) if analysis['longitude'] is not None else None
                        },
                        "bounds": {
                            "lat_min": float(analysis['lat_min']) if analysis['lat_min'] is not None else None,
                            "lat_max": float(analysis['lat_max']) if analysis['lat_max'] is not None else None,
                            "lon_min": float(analysis['lon_min']) if analysis['lon_min'] is not None else None,
                            "lon_max": float(analysis['lon_max']) if analysis['lon_max'] is not None else None
                        }
                    },
                    "scientific_explanation": analysis['scientific_explanation'],
                    "explanation_type": analysis['explanation_type'],
                    "created_at": analysis['created_at'].isoformat()
                },
                "measurements": [
                    {
                        "instrument_name": row['instrument_name'],
                        "value": float(row['value']),
                        "unit": row['unit'],
                        "data_mode": row['data_mode'],
                        "source": row['source'],
                        "latitude": float(row['latitude']),
                        "longitude": float(row['longitude']),
                        "timestamp": row['measurement_timestamp'].isoformat()
                    }
                    for row in measurements
                ],
                "failed_instruments": [
                    {
                        "instrument_name": row['instrument_name'],
                        "reason": "NO_DATA",
                        "source": row['source'],
                        "timestamp": row['measurement_timestamp'].isoformat()
                    }
                    for row in failed_instruments
                ]
            }
    except HTTPException:
        raise
    except Exception as e:
        print(f"[ERROR] Error consultando an√°lisis {analysis_id}: {e}", flush=True)
        raise HTTPException(status_code=500, detail=f"Error consultando an√°lisis: {str(e)}")

@router.get("/analyses/by-region/{region_name}", summary="Obtener an√°lisis por regi√≥n")
async def get_analyses_by_region(region_name: str, limit: int = 10):
    """
    # Consultar An√°lisis por Regi√≥n
    
    Retorna todos los an√°lisis realizados en una regi√≥n espec√≠fica.
    
    ## Par√°metros
    
    - `region_name` (requerido): Nombre de la regi√≥n (ej: "Groenlandia Test", "Sahara Norte")
    - `limit` (opcional): N√∫mero m√°ximo de an√°lisis a retornar (default: 10)
    
    ## Respuesta
    
    Objeto con:
    - `region`: Nombre de la regi√≥n consultada
    - `total`: N√∫mero de an√°lisis encontrados
    - `analyses`: Lista de an√°lisis ordenados por fecha
    
    ## Uso
    
    √ötil para:
    - Comparar m√∫ltiples an√°lisis de la misma regi√≥n
    - Evaluar cambios temporales
    - Validar consistencia de resultados
    - An√°lisis de series temporales
    
    ## Ejemplo
    
    ```
    GET /api/scientific/analyses/by-region/Groenlandia%20Test?limit=5
    ```
    """
    if not db_pool:
        raise HTTPException(status_code=503, detail="Base de datos no disponible")
    
    try:
        async with db_pool.acquire() as conn:
            analyses = await conn.fetch("""
                SELECT 
                    id,
                    candidate_name,
                    region,
                    archaeological_probability,
                    anomaly_score,
                    result_type,
                    recommended_action,
                    environment_type,
                    confidence_level,
                    created_at
                FROM archaeological_candidate_analyses
                WHERE region = $1
                ORDER BY created_at DESC
                LIMIT $2
            """, region_name, limit)
            
            return {
                "region": region_name,
                "total": len(analyses),
                "analyses": [
                    {
                        "id": row['id'],
                        "candidate_name": row['candidate_name'],
                        "archaeological_probability": float(row['archaeological_probability']),
                        "anomaly_score": float(row['anomaly_score']),
                        "result_type": row['result_type'],
                        "recommended_action": row['recommended_action'],
                        "environment_type": row['environment_type'],
                        "confidence_level": float(row['confidence_level']),
                        "created_at": row['created_at'].isoformat()
                    }
                    for row in analyses
                ]
            }
    except Exception as e:
        print(f"[ERROR] Error consultando an√°lisis de regi√≥n {region_name}: {e}", flush=True)
        raise HTTPException(status_code=500, detail=f"Error consultando an√°lisis: {str(e)}")


@router.get("/sites/all", summary="Listar todos los sitios arqueol√≥gicos")
async def get_all_archaeological_sites(
    page: int = 1,
    page_size: int = 100,
    country: Optional[str] = None,
    site_type: Optional[str] = None,
    environment_type: Optional[str] = None,
    confidence_level: Optional[str] = None,
    search: Optional[str] = None
):
    """
    # Listar Todos los Sitios Arqueol√≥gicos
    
    Retorna todos los sitios arqueol√≥gicos de la base de datos con paginaci√≥n y filtros.
    
    ## Par√°metros de Paginaci√≥n
    
    - `page`: N√∫mero de p√°gina (default: 1)
    - `page_size`: Tama√±o de p√°gina (default: 100, max: 1000)
    
    ## Filtros Opcionales
    
    - `country`: Filtrar por pa√≠s (ej: "M√©xico", "Per√∫")
    - `site_type`: Filtrar por tipo de sitio (SETTLEMENT, MONUMENT, BURIAL, etc.)
    - `environment_type`: Filtrar por ambiente (DESERT, MOUNTAIN, FOREST, etc.)
    - `confidence_level`: Filtrar por nivel de confianza (CONFIRMED, PROBABLE, POSSIBLE)
    - `search`: B√∫squeda por nombre (case-insensitive)
    
    ## Respuesta
    
    Retorna:
    - `total`: Total de sitios en la BD
    - `page`: P√°gina actual
    - `page_size`: Tama√±o de p√°gina
    - `total_pages`: Total de p√°ginas
    - `sites`: Array de sitios con informaci√≥n completa
    
    ## Ejemplo
    
    ```
    GET /api/scientific/sites/all?page=1&page_size=50&country=M√©xico
    ```
    """
    
    if not db_pool:
        raise HTTPException(status_code=503, detail="Base de datos no disponible")
    
    # Validar page_size
    if page_size > 1000:
        page_size = 1000
    if page_size < 1:
        page_size = 100
    
    if page < 1:
        page = 1
    
    try:
        async with db_pool.acquire() as conn:
            # Construir query con filtros
            where_clauses = []
            params = []
            param_count = 1
            
            if country:
                where_clauses.append(f"country = ${param_count}")
                params.append(country)
                param_count += 1
            
            if site_type:
                where_clauses.append(f'"siteType" = ${param_count}')
                params.append(site_type)
                param_count += 1
            
            if environment_type:
                where_clauses.append(f'"environmentType" = ${param_count}')
                params.append(environment_type)
                param_count += 1
            
            if confidence_level:
                where_clauses.append(f'"confidenceLevel" = ${param_count}')
                params.append(confidence_level)
                param_count += 1
            
            if search:
                where_clauses.append(f"name ILIKE ${param_count}")
                params.append(f"%{search}%")
                param_count += 1
            
            where_sql = " AND ".join(where_clauses) if where_clauses else "TRUE"
            
            # Contar total de sitios (con filtros)
            count_query = f"SELECT COUNT(*) FROM archaeological_sites WHERE {where_sql}"
            total = await conn.fetchval(count_query, *params)
            
            # Calcular offset
            offset = (page - 1) * page_size
            
            # Obtener sitios paginados
            sites_query = f"""
                SELECT 
                    id,
                    name,
                    slug,
                    "siteType",
                    "environmentType",
                    "confidenceLevel",
                    latitude,
                    longitude,
                    country,
                    region,
                    description,
                    "scientificSignificance",
                    "isControlSite",
                    "discoveryDate",
                    "createdAt",
                    "updatedAt"
                FROM archaeological_sites
                WHERE {where_sql}
                ORDER BY "createdAt" DESC
                LIMIT ${param_count} OFFSET ${param_count + 1}
            """
            params.extend([page_size, offset])
            
            sites = await conn.fetch(sites_query, *params)
            
            # Calcular total de p√°ginas
            total_pages = (total + page_size - 1) // page_size
            
            return {
                "total": total,
                "page": page,
                "page_size": page_size,
                "total_pages": total_pages,
                "filters": {
                    "country": country,
                    "site_type": site_type,
                    "environment_type": environment_type,
                    "confidence_level": confidence_level,
                    "search": search
                },
                "sites": [
                    {
                        "id": str(row['id']),
                        "name": row['name'],
                        "slug": row['slug'],
                        "site_type": row['siteType'],
                        "environment_type": row['environmentType'],
                        "confidence_level": row['confidenceLevel'],
                        "coordinates": {
                            "latitude": float(row['latitude']) if row['latitude'] is not None else None,
                            "longitude": float(row['longitude']) if row['longitude'] is not None else None
                        },
                        "location": {
                            "country": row['country'],
                            "region": row['region']
                        },
                        "description": row['description'],
                        "scientific_significance": row['scientificSignificance'],
                        "is_control_site": row['isControlSite'],
                        "discovery_date": row['discoveryDate'].isoformat() if row['discoveryDate'] else None,
                        "created_at": row['createdAt'].isoformat() if row['createdAt'] else None,
                        "updated_at": row['updatedAt'].isoformat() if row['updatedAt'] else None
                    }
                    for row in sites
                ]
            }
            
    except Exception as e:
        print(f"[ERROR] Error listando sitios: {e}", flush=True)
        raise HTTPException(status_code=500, detail=f"Error listando sitios: {str(e)}")


@router.get("/sites/stats", summary="Estad√≠sticas de sitios arqueol√≥gicos")
async def get_sites_statistics():
    """
    # Estad√≠sticas de Sitios Arqueol√≥gicos
    
    Retorna estad√≠sticas agregadas de todos los sitios en la base de datos.
    
    ## Respuesta
    
    Retorna:
    - `total_sites`: Total de sitios en la BD
    - `by_country`: Distribuci√≥n por pa√≠s (top 20)
    - `by_site_type`: Distribuci√≥n por tipo de sitio
    - `by_environment`: Distribuci√≥n por tipo de ambiente
    - `by_confidence`: Distribuci√≥n por nivel de confianza
    - `control_sites`: N√∫mero de sitios de control
    - `recent_additions`: Sitios agregados en los √∫ltimos 7 d√≠as
    """
    
    if not db_pool:
        raise HTTPException(status_code=503, detail="Base de datos no disponible")
    
    try:
        async with db_pool.acquire() as conn:
            # Total de sitios
            total = await conn.fetchval('SELECT COUNT(*) FROM archaeological_sites')
            
            # Por pa√≠s (top 20)
            by_country = await conn.fetch("""
                SELECT country, COUNT(*) as count
                FROM archaeological_sites
                WHERE country IS NOT NULL
                GROUP BY country
                ORDER BY count DESC
                LIMIT 20
            """)
            
            # Por tipo de sitio
            by_site_type = await conn.fetch("""
                SELECT "siteType", COUNT(*) as count
                FROM archaeological_sites
                GROUP BY "siteType"
                ORDER BY count DESC
            """)
            
            # Por ambiente
            by_environment = await conn.fetch("""
                SELECT "environmentType", COUNT(*) as count
                FROM archaeological_sites
                GROUP BY "environmentType"
                ORDER BY count DESC
            """)
            
            # Por confianza
            by_confidence = await conn.fetch("""
                SELECT "confidenceLevel", COUNT(*) as count
                FROM archaeological_sites
                GROUP BY "confidenceLevel"
                ORDER BY count DESC
            """)
            
            # Sitios de control
            control_sites = await conn.fetchval("""
                SELECT COUNT(*) FROM archaeological_sites
                WHERE "isControlSite" = TRUE
            """)
            
            # Adiciones recientes (√∫ltimos 7 d√≠as)
            recent = await conn.fetchval("""
                SELECT COUNT(*) FROM archaeological_sites
                WHERE "createdAt" >= NOW() - INTERVAL '7 days'
            """)
            
            return {
                "total_sites": total,
                "by_country": [
                    {"country": row['country'], "count": row['count']}
                    for row in by_country
                ],
                "by_site_type": [
                    {"site_type": row['siteType'], "count": row['count']}
                    for row in by_site_type
                ],
                "by_environment": [
                    {"environment_type": row['environmentType'], "count": row['count']}
                    for row in by_environment
                ],
                "by_confidence": [
                    {"confidence_level": row['confidenceLevel'], "count": row['count']}
                    for row in by_confidence
                ],
                "control_sites": control_sites,
                "recent_additions": recent
            }
            
    except Exception as e:
        print(f"[ERROR] Error obteniendo estad√≠sticas: {e}", flush=True)
        raise HTTPException(status_code=500, detail=f"Error obteniendo estad√≠sticas: {str(e)}")


# ============================================================================
# ENDPOINTS PARA CAPA DE SITIOS ARQUEOL√ìGICOS
# ============================================================================

@router.get("/sites/layer")
async def get_sites_layer(
    confidence_level: Optional[str] = None,
    site_type: Optional[str] = None,
    country: Optional[str] = None,
    limit: int = 2000,  # REDUCIDO: 10000 ‚Üí 2000 para mejor UX
    # FASE 2: Filtrado espacial (bbox)
    bbox: Optional[str] = None  # "lat_min,lon_min,lat_max,lon_max"
):
    """
    Obtener sitios para capa de mapa (GeoJSON).
    
    Par√°metros:
    - confidence_level: HIGH, MODERATE, LOW, CANDIDATE
    - site_type: Filtrar por tipo
    - country: Filtrar por pa√≠s
    - limit: M√°ximo de sitios (default 2000, max 10000)
    - bbox: Bounding box "lat_min,lon_min,lat_max,lon_max" (opcional)
    
    Returns:
        GeoJSON FeatureCollection con sitios filtrados
    """
    
    if not db_pool:
        raise HTTPException(status_code=500, detail="Database not available")
    
    # FASE 1: Limitar resultados para mejor UX
    if limit > 10000:
        limit = 10000
        print(f"‚ö†Ô∏è WARNING: Limit capped at 10000 for performance")
    
    try:
        async with db_pool.acquire() as conn:
            # Construir query
            where_clauses = []
            params = []
            param_count = 1
            
            if confidence_level:
                where_clauses.append(f'"confidenceLevel" = ${param_count}')
                params.append(confidence_level)
                param_count += 1
            
            if site_type:
                where_clauses.append(f'"siteType" = ${param_count}')
                params.append(site_type)
                param_count += 1
            
            if country:
                where_clauses.append(f'country = ${param_count}')
                params.append(country)
                param_count += 1
            
            # FASE 2: Filtrado por bounding box
            if bbox:
                try:
                    lat_min, lon_min, lat_max, lon_max = map(float, bbox.split(','))
                    where_clauses.append(f'latitude BETWEEN ${param_count} AND ${param_count + 1}')
                    where_clauses.append(f'longitude BETWEEN ${param_count + 2} AND ${param_count + 3}')
                    params.extend([lat_min, lat_max, lon_min, lon_max])
                    param_count += 4
                    print(f"üìç Spatial filter applied: bbox={bbox}")
                except ValueError:
                    print(f"‚ö†Ô∏è WARNING: Invalid bbox format '{bbox}', ignoring spatial filter")
            
            where_sql = " AND ".join(where_clauses) if where_clauses else "TRUE"
            
            # Contar total antes de limitar
            count_query = f"""
                SELECT COUNT(*) as total
                FROM archaeological_sites
                WHERE {where_sql}
            """
            
            total_count = await conn.fetchval(count_query, *params)
            
            query = f"""
                SELECT 
                    id,
                    name,
                    slug,
                    "siteType" as site_type,
                    "environmentType" as environment_type,
                    "confidenceLevel" as confidence_level,
                    latitude,
                    longitude,
                    country,
                    region,
                    description,
                    "createdAt" as created_at
                FROM archaeological_sites
                WHERE {where_sql}
                ORDER BY "confidenceLevel" DESC, "createdAt" DESC
                LIMIT ${param_count}
            """
            params.append(limit)
            
            sites = await conn.fetch(query, *params)
            
            # FASE 1: Warning si se trunca
            if total_count > limit:
                print(f"‚ö†Ô∏è WARNING: Results truncated - showing {limit} of {total_count} sites")
            
            # Convertir a GeoJSON
            features = []
            for site in sites:
                feature = {
                    "type": "Feature",
                    "geometry": {
                        "type": "Point",
                        "coordinates": [site['longitude'], site['latitude']]
                    },
                    "properties": {
                        "id": str(site['id']),
                        "name": site['name'],
                        "slug": site['slug'],
                        "siteType": site['site_type'],
                        "environmentType": site['environment_type'],
                        "confidenceLevel": site['confidence_level'],
                        "country": site['country'],
                        "region": site['region'],
                        "description": site['description'][:200] if site['description'] else "",
                        "createdAt": site['created_at'].isoformat() if site['created_at'] else None
                    }
                }
                features.append(feature)
            
            geojson = {
                "type": "FeatureCollection",
                "features": features,
                "metadata": {
                    "total": len(features),
                    "total_available": total_count,  # NUEVO: total sin l√≠mite
                    "truncated": total_count > limit,  # NUEVO: indica si se trunc√≥
                    "spatial_filtered": bbox is not None,  # NUEVO: indica si hay filtro espacial
                    "filters": {
                        "confidence_level": confidence_level,
                        "site_type": site_type,
                        "country": country,
                        "bbox": bbox,
                        "limit": limit
                    }
                }
            }
            
            return geojson
            
    except Exception as e:
        print(f"Error getting sites layer: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/sites/candidate")
async def add_candidate_site(request: dict):
    """
    Agregar nuevo sitio candidato a la capa.
    
    Body:
    {
        "name": "Candidato Amazon√≠a 001",
        "latitude": -10.5,
        "longitude": -70.2,
        "country": "Brazil",
        "region": "Acre",
        "origin_probability": 0.85,
        "activity_probability": 0.05,
        "anomaly_probability": 0.02,
        "ess": "high",
        "ess_score": 0.75,
        "description": "Candidato detectado...",
        "analysis_id": "uuid"
    }
    """
    
    if not db_pool:
        raise HTTPException(status_code=500, detail="Database not available")
    
    try:
        # Validar datos requeridos
        required = ['name', 'latitude', 'longitude', 'country']
        for field in required:
            if field not in request:
                raise HTTPException(status_code=400, detail=f"Missing field: {field}")
        
        # Generar slug
        import re
        slug_base = f"{request['name']}-{request['latitude']:.4f}-{request['longitude']:.4f}"
        slug = re.sub(r'[^a-z0-9-]', '', slug_base.lower().replace(' ', '-'))
        
        # Generar descripci√≥n con m√©tricas separadas
        origin = request.get('origin_probability', 0)
        activity = request.get('activity_probability', 0)
        anomaly = request.get('anomaly_probability', 0)
        ess = request.get('ess', 'none')
        
        description = (
            f"Candidato arqueol√≥gico detectado por ArcheoScope. "
            f"M√©tricas: Origen {origin:.0%}, Actividad {activity:.0%}, "
            f"Anomal√≠a {anomaly:.0%}. ESS: {ess.upper()}. "
            f"Requiere validaci√≥n de campo."
        )
        
        if 'description' in request:
            description = request['description']
        
        async with db_pool.acquire() as conn:
            # Insertar sitio
            site_id = await conn.fetchval("""
                INSERT INTO archaeological_sites (
                    name,
                    slug,
                    "siteType",
                    "environmentType",
                    "confidenceLevel",
                    latitude,
                    longitude,
                    country,
                    region,
                    description,
                    "createdAt",
                    "updatedAt"
                ) VALUES (
                    $1, $2, $3, $4, $5, $6, $7, $8, $9, $10, NOW(), NOW()
                )
                RETURNING id
            """,
                request['name'],
                slug,
                'UNKNOWN',  # Tipo por defecto
                request.get('environment_type', 'UNKNOWN'),
                'CANDIDATE',  # Siempre candidato
                request['latitude'],
                request['longitude'],
                request['country'],
                request.get('region', ''),
                description
            )
            
            return {
                "success": True,
                "site_id": str(site_id),
                "message": "Candidato agregado a la capa",
                "slug": slug
            }
            
    except Exception as e:
        print(f"Error adding candidate: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/sites/candidates")
async def get_candidates_only(limit: int = 1000):
    """
    Obtener solo sitios CANDIDATOS para revisi√≥n.
    
    Returns:
        Lista de candidatos con m√©tricas extra√≠das
    """
    
    if not db_pool:
        raise HTTPException(status_code=500, detail="Database not available")
    
    try:
        async with db_pool.acquire() as conn:
            candidates = await conn.fetch("""
                SELECT 
                    id,
                    name,
                    slug,
                    "siteType" as site_type,
                    "environmentType" as environment_type,
                    latitude,
                    longitude,
                    country,
                    region,
                    description,
                    "createdAt" as created_at
                FROM archaeological_sites
                WHERE "confidenceLevel" = 'CANDIDATE'
                ORDER BY "createdAt" DESC
                LIMIT $1
            """, limit)
            
            result = []
            for c in candidates:
                # Extraer m√©tricas de la descripci√≥n
                import re
                desc = c['description']
                
                origin = 0.0
                activity = 0.0
                anomaly = 0.0
                ess = "none"
                
                # Buscar m√©tricas en descripci√≥n
                origin_match = re.search(r'Origen (\d+)%', desc)
                if origin_match:
                    origin = float(origin_match.group(1)) / 100
                
                activity_match = re.search(r'Actividad (\d+)%', desc)
                if activity_match:
                    activity = float(activity_match.group(1)) / 100
                
                anomaly_match = re.search(r'Anomal√≠a (\d+)%', desc)
                if anomaly_match:
                    anomaly = float(anomaly_match.group(1)) / 100
                
                ess_match = re.search(r'ESS: (\w+)', desc)
                if ess_match:
                    ess = ess_match.group(1).lower()
                
                result.append({
                    "id": str(c['id']),
                    "name": c['name'],
                    "slug": c['slug'],
                    "site_type": c['site_type'],
                    "environment_type": c['environment_type'],
                    "latitude": c['latitude'],
                    "longitude": c['longitude'],
                    "country": c['country'],
                    "region": c['region'],
                    "description": c['description'],
                    "created_at": c['created_at'].isoformat() if c['created_at'] else None,
                    "metrics": {
                        "origin": origin,
                        "activity": activity,
                        "anomaly": anomaly,
                        "ess": ess
                    }
                })
            
            return {
                "total": len(result),
                "candidates": result
            }
            
    except Exception as e:
        print(f"Error getting candidates: {e}")
        raise HTTPException(status_code=500, detail=str(e))
