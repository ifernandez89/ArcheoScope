#!/usr/bin/env python3
"""
Endpoint CientÃ­fico de AnÃ¡lisis - ArcheoScope
==============================================

Implementa el pipeline cientÃ­fico completo de 7 fases (0, A-F, G).
"""

from fastapi import APIRouter, HTTPException
from pydantic import BaseModel
from typing import Dict, List, Any, Optional
import sys
from pathlib import Path
import os

# AÃ±adir backend al path
sys.path.insert(0, str(Path(__file__).parent.parent))

from scientific_pipeline import ScientificPipeline
from satellite_connectors.real_data_integrator import RealDataIntegrator
from environment_classifier import EnvironmentClassifier
from validation.real_archaeological_validator import RealArchaeologicalValidator
import asyncpg

router = APIRouter()

# Inicializar componentes
integrator = RealDataIntegrator()
classifier = EnvironmentClassifier()
validator = RealArchaeologicalValidator()

# Pool de conexiones a BD (se inicializa en startup)
db_pool = None

async def init_db_pool():
    """Inicializar pool de conexiones a BD."""
    global db_pool
    database_url = os.getenv("DATABASE_URL")
    if database_url:
        try:
            db_pool = await asyncpg.create_pool(database_url, min_size=2, max_size=10)
            print("[SCIENTIFIC_ENDPOINT] Pool de BD inicializado", flush=True)
        except Exception as e:
            print(f"[SCIENTIFIC_ENDPOINT] Error inicializando pool: {e}", flush=True)
            db_pool = None
    else:
        print("[SCIENTIFIC_ENDPOINT] DATABASE_URL no configurada", flush=True)

class ScientificAnalysisRequest(BaseModel):
    """Solicitud de anÃ¡lisis cientÃ­fico."""
    lat_min: float
    lat_max: float
    lon_min: float
    lon_max: float
    region_name: str
    candidate_id: Optional[str] = None

@router.post("/analyze")
async def analyze_scientific(request: ScientificAnalysisRequest):
    """
    # AnÃ¡lisis CientÃ­fico Completo - Pipeline de 7 Fases
    
    Ejecuta el pipeline cientÃ­fico determinÃ­stico completo para anÃ¡lisis arqueolÃ³gico remoto.
    
    ## Fases del Pipeline
    
    - **Fase 0**: Enriquecimiento con datos histÃ³ricos de BD
    - **Fase A**: NormalizaciÃ³n por instrumento
    - **Fase B**: DetecciÃ³n de anomalÃ­a pura
    - **Fase C**: AnÃ¡lisis morfolÃ³gico explÃ­cito
    - **Fase D**: Inferencia antropogÃ©nica (con freno de mano)
    - **Fase E**: VerificaciÃ³n de anti-patrones
    - **Fase F**: ValidaciÃ³n contra sitios conocidos
    - **Fase G**: Salida cientÃ­fica
    
    ## CaracterÃ­sticas
    
    - âœ… 100% DeterminÃ­stico y reproducible
    - âœ… Mediciones con instrumentos reales (Sentinel, Landsat, ICESat-2, etc.)
    - âœ… Guardado automÃ¡tico en base de datos
    - âœ… Etiquetado epistemolÃ³gico completo
    - âœ… Sin uso de IA en decisiones cientÃ­ficas
    
    ## ParÃ¡metros
    
    - `lat_min`, `lat_max`: Rango de latitud (grados decimales)
    - `lon_min`, `lon_max`: Rango de longitud (grados decimales)
    - `region_name`: Nombre descriptivo de la regiÃ³n
    - `candidate_id` (opcional): ID personalizado del candidato
    
    ## Respuesta
    
    Retorna anÃ¡lisis completo con:
    - Salida cientÃ­fica (probabilidad antropogÃ©nica, anomaly score, acciÃ³n recomendada)
    - Contexto ambiental (tipo de ambiente, visibilidad arqueolÃ³gica)
    - Mediciones instrumentales (valores, fuentes, modos de datos)
    - InformaciÃ³n de la solicitud (coordenadas, regiÃ³n)
    
    ## Ejemplo
    
    ```json
    {
      "lat_min": 64.19,
      "lat_max": 64.21,
      "lon_min": -51.71,
      "lon_max": -51.69,
      "region_name": "Groenlandia Test"
    }
    ```
    """
    
    print("\n" + "="*80, flush=True)
    print("ENDPOINT /analyze-scientific ALCANZADO", flush=True)
    print(f"RegiÃ³n: {request.region_name}", flush=True)
    print(f"Bounds: [{request.lat_min}, {request.lat_max}] x [{request.lon_min}, {request.lon_max}]", flush=True)
    print("="*80 + "\n", flush=True)
    
    try:
        # Inicializar pipeline con BD y validator
        pipeline = ScientificPipeline(db_pool=db_pool, validator=validator)
        
        # Calcular centro
        center_lat = (request.lat_min + request.lat_max) / 2
        center_lon = (request.lon_min + request.lon_max) / 2
        
        # 1. Clasificar ambiente
        print("[STEP 1] Clasificando ambiente...", flush=True)
        env_context = classifier.classify(center_lat, center_lon)
        print(f"  Ambiente: {env_context.environment_type.value}", flush=True)
        print(f"  Confianza: {env_context.confidence:.2f}", flush=True)
        
        # 2. Medir con instrumentos reales
        print("[STEP 2] Midiendo con TODOS los instrumentos disponibles para el ambiente...", flush=True)
        
        # USAR TODOS LOS INSTRUMENTOS DISPONIBLES (primarios + secundarios)
        # No hay jerarquÃ­a - todos son igualmente importantes
        all_instruments = list(set(env_context.primary_sensors + env_context.secondary_sensors))
        print(f"  Total instrumentos disponibles para {env_context.environment_type.value}: {len(all_instruments)}", flush=True)
        print(f"  Instrumentos: {', '.join(all_instruments)}", flush=True)
        
        measurements = []
        for instrument_name in all_instruments:
            try:
                measurement = await integrator.get_instrument_measurement(
                    instrument_name=instrument_name,
                    lat_min=request.lat_min,
                    lat_max=request.lat_max,
                    lon_min=request.lon_min,
                    lon_max=request.lon_max
                )
                # Solo agregar si la mediciÃ³n es vÃ¡lida (no None)
                if measurement is not None:
                    measurements.append(measurement)
                    print(f"  âœ… {instrument_name}: {measurement.get('value', 0):.3f}", flush=True)
                else:
                    print(f"  âŒ {instrument_name}: Sin datos", flush=True)
            except Exception as e:
                print(f"  âŒ {instrument_name}: Error - {e}", flush=True)
                continue
        
        print(f"\n  ðŸ“Š RESUMEN: {len(measurements)}/{len(all_instruments)} instrumentos midieron exitosamente", flush=True)
        if len(measurements) > 0:
            print(f"  Instrumentos exitosos:", flush=True)
            for m in measurements:
                if m is not None:
                    print(f"    - {m.get('instrument_name', 'unknown')}: {m.get('value', 0):.3f} ({m.get('data_mode', 'unknown')})", flush=True)
        
        # 3. Preparar datos para pipeline
        raw_measurements = {
            'candidate_id': request.candidate_id or f"{request.region_name}_{center_lat:.4f}_{center_lon:.4f}",
            'region_name': request.region_name,
            'center_lat': center_lat,
            'center_lon': center_lon,
            'environment_type': env_context.environment_type.value
        }
        
        # AÃ±adir mediciones (measurements son diccionarios)
        for m in measurements:
            if m is not None:
                instrument_name = m.get('instrument_name', 'unknown')
                raw_measurements[instrument_name] = {
                    'value': m.get('value', 0),
                    'threshold': m.get('threshold', 0),
                    'exceeds_threshold': m.get('exceeds_threshold', False),
                    'confidence': m.get('confidence', 0),
                    'data_mode': m.get('data_mode', 'unknown'),
                    'source': m.get('source', 'unknown')
                }
        
        # 4. Ejecutar pipeline cientÃ­fico (ASYNC con enriquecimiento de BD)
        print("[STEP 3] Ejecutando pipeline cientÃ­fico...", flush=True)
        result = await pipeline.analyze(
            raw_measurements,
            request.lat_min, request.lat_max,
            request.lon_min, request.lon_max
        )
        
        # 5. AÃ±adir contexto adicional
        result['environment_context'] = {
            'environment_type': env_context.environment_type.value,
            'confidence': env_context.confidence,
            'available_instruments': list(set(env_context.primary_sensors + env_context.secondary_sensors)),  # TODOS los disponibles
            'archaeological_visibility': env_context.archaeological_visibility,
            'preservation_potential': env_context.preservation_potential
        }
        
        result['instrumental_measurements'] = [
            {
                'instrument_name': m.get('instrument_name', 'unknown'),
                'value': m.get('value', 0),
                'threshold': m.get('threshold', 0),
                'exceeds_threshold': m.get('exceeds_threshold', False),
                'confidence': m.get('confidence', 0),
                'data_mode': m.get('data_mode', 'unknown'),
                'source': m.get('source', 'unknown')
            }
            for m in measurements if m is not None  # Filtrar None
        ]
        
        result['request_info'] = {
            'region_name': request.region_name,
            'center_lat': center_lat,
            'center_lon': center_lon,
            'bounds': {
                'lat_min': request.lat_min,
                'lat_max': request.lat_max,
                'lon_min': request.lon_min,
                'lon_max': request.lon_max
            }
        }
        
        print("\n[SUCCESS] AnÃ¡lisis cientÃ­fico completado", flush=True)
        print(f"  Anomaly score: {result['scientific_output']['anomaly_score']:.3f}", flush=True)
        print(f"  Anthropic probability: {result['scientific_output']['anthropic_probability']:.3f}", flush=True)
        print(f"  Recommended action: {result['scientific_output']['recommended_action']}", flush=True)
        
        # 6. GUARDAR RESULTADOS EN BD (ESTRUCTURA COMPLETA)
        if db_pool:
            try:
                print("\n[BD] Guardando resultados en base de datos...", flush=True)
                
                # Importar generador de nombres
                from site_name_generator import site_name_generator
                
                # Generar nombre descriptivo del sitio
                site_info = site_name_generator.generate_name(
                    center_lat, 
                    center_lon, 
                    env_context.environment_type.value
                )
                
                print(f"[BD] Nombre generado: {site_info['name']}", flush=True)
                print(f"[BD] PaÃ­s: {site_info['country']}, RegiÃ³n: {site_info['region']}", flush=True)
                
                # Mapear environment type a ENUM de BD
                env_type_mapping = {
                    'desert': 'DESERT',
                    'semi_arid': 'SEMI_ARID',
                    'forest': 'FOREST',
                    'tropical_forest': 'FOREST',
                    'grassland': 'GRASSLAND',
                    'mountain': 'MOUNTAIN',
                    'glacier': 'GLACIER',
                    'polar_ice': 'POLAR_ICE',
                    'permafrost': 'PERMAFROST',
                    'shallow_sea': 'SHALLOW_SEA',
                    'deep_ocean': 'DEEP_OCEAN',
                    'coastal': 'COASTAL',
                    'lake': 'LAKE',
                    'river': 'RIVER',
                    'agricultural': 'AGRICULTURAL',
                    'urban': 'URBAN',
                    'unknown': 'UNKNOWN'
                }
                
                env_type_db = env_type_mapping.get(
                    env_context.environment_type.value, 
                    'UNKNOWN'
                )
                
                async with db_pool.acquire() as conn:
                    # 1. GUARDAR EN archaeological_sites (MISMA ESTRUCTURA QUE LOS 80K)
                    site_id = await conn.fetchval("""
                        INSERT INTO archaeological_sites 
                        (name, slug, "environmentType", "siteType", "confidenceLevel", 
                         "excavationStatus", "preservationStatus", latitude, longitude,
                         country, region, description, "scientificSignificance",
                         "isReferencesite", "isControlSite", "discoveryDate")
                        VALUES ($1, $2, $3::text::"EnvironmentType", $4::text::"SiteType", 
                                $5::text::"ConfidenceLevel", $6::text::"ExcavationStatus", 
                                $7::text::"PreservationStatus", $8, $9, $10, $11, $12, $13, $14, $15, NOW())
                        RETURNING id
                    """,
                        site_info['name'],
                        site_info['slug'],
                        env_type_db,
                        'UNKNOWN',  # siteType: UNKNOWN hasta clasificaciÃ³n
                        'CANDIDATE',  # confidenceLevel: CANDIDATE para nuevos sitios
                        'UNEXCAVATED',  # excavationStatus
                        'UNKNOWN',  # preservationStatus
                        center_lat,
                        center_lon,
                        site_info['country'],
                        site_info['region'],
                        f"Candidato detectado por ArcheoScope. Probabilidad antropogÃ©nica: {result['scientific_output']['anthropic_probability']:.3f}",
                        f"Anomaly score: {result['scientific_output']['anomaly_score']:.3f}. "
                        f"Instrumentos: {len(measurements)}/{len(all_instruments)}. "
                        f"AcciÃ³n recomendada: {result['scientific_output']['recommended_action']}",
                        False,  # isReferencesite
                        result['scientific_output']['candidate_type'] == 'negative_reference'  # isControlSite
                    )
                    
                    print(f"[BD] âœ… Sitio guardado con ID: {site_id}", flush=True)
                    
                    # 2. GUARDAR EN archaeological_candidate_analyses (anÃ¡lisis detallado)
                    await conn.execute("""
                        INSERT INTO archaeological_candidate_analyses 
                        (candidate_id, candidate_name, region, archaeological_probability, anomaly_score, 
                         result_type, recommended_action, environment_type, confidence_level,
                         instruments_measuring, instruments_total)
                        VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11)
                    """, 
                        site_id,  # Usar ID del sitio
                        site_info['name'],
                        request.region_name,
                        result['scientific_output']['anthropic_probability'],
                        result['scientific_output']['anomaly_score'],
                        result['scientific_output']['candidate_type'],
                        result['scientific_output']['recommended_action'],
                        env_context.environment_type.value,
                        result['scientific_output']['confidence_interval'][0],  # Lower bound
                        len(measurements),  # Instrumentos que midieron
                        len(all_instruments)  # Total instrumentos disponibles
                    )
                    
                    # 3. GUARDAR MEDICIONES INSTRUMENTALES (exitosas)
                    for m in measurements:
                        if m is not None:
                            await conn.execute("""
                                INSERT INTO measurements 
                                (instrument_name, measurement_type, value, unit, data_mode, source, 
                                 latitude, longitude, region_name, environment_type)
                                VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10)
                            """,
                                m.get('instrument_name', 'unknown'),
                                'remote_sensing',
                                m.get('value', 0),
                                'various',
                                m.get('data_mode', 'unknown'),
                                m.get('source', 'unknown'),
                                center_lat,
                                center_lon,
                                request.region_name,
                                env_context.environment_type.value
                            )
                    
                    # 4. GUARDAR INSTRUMENTOS FALLIDOS (los que no midieron)
                    failed_instruments = set(all_instruments) - set([m.get('instrument_name') for m in measurements if m])
                    for instrument_name in failed_instruments:
                        await conn.execute("""
                            INSERT INTO measurements 
                            (instrument_name, measurement_type, value, unit, data_mode, source,
                             latitude, longitude, region_name, environment_type)
                            VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10)
                        """,
                            instrument_name,
                            'remote_sensing',
                            0.0,  # Valor 0 para fallidos
                            'none',
                            'NO_DATA',  # Marcar como sin datos
                            'failed',
                            center_lat,
                            center_lon,
                            request.region_name,
                            env_context.environment_type.value
                        )
                    
                    print(f"[BD] âœ… Guardado completo:", flush=True)
                    print(f"     - 1 sitio arqueolÃ³gico (ID: {site_id})", flush=True)
                    print(f"     - 1 anÃ¡lisis cientÃ­fico", flush=True)
                    print(f"     - {len(measurements)} mediciones exitosas", flush=True)
                    print(f"     - {len(failed_instruments)} instrumentos fallidos registrados", flush=True)
                    
            except Exception as e:
                print(f"[BD] âš ï¸ Error guardando en BD: {e}", flush=True)
                import traceback
                traceback.print_exc()
                # No fallar el anÃ¡lisis si falla el guardado
                # No fallar el anÃ¡lisis si falla el guardado
        else:
            print("[BD] âš ï¸ Sin conexiÃ³n a BD - resultados no persistidos", flush=True)
        
        return result
        
    except Exception as e:
        print(f"\n[ERROR] Error en anÃ¡lisis cientÃ­fico: {e}", flush=True)
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"Error en anÃ¡lisis cientÃ­fico: {str(e)}")


@router.get("/analyses/recent", summary="Obtener anÃ¡lisis recientes")
async def get_recent_analyses(limit: int = 10):
    """
    # Consultar AnÃ¡lisis Recientes
    
    Retorna los Ãºltimos N anÃ¡lisis realizados, ordenados por fecha (mÃ¡s reciente primero).
    
    ## ParÃ¡metros
    
    - `limit` (opcional): NÃºmero mÃ¡ximo de anÃ¡lisis a retornar (default: 10, mÃ¡ximo: 100)
    
    ## Respuesta
    
    Lista de anÃ¡lisis con:
    - ID Ãºnico del anÃ¡lisis
    - Nombre del candidato
    - RegiÃ³n analizada
    - Probabilidad antropogÃ©nica
    - Anomaly score
    - Tipo de resultado (positive_candidate, negative_reference, uncertain)
    - AcciÃ³n recomendada
    - Tipo de ambiente
    - Nivel de confianza
    - Fecha de creaciÃ³n
    
    ## Uso
    
    Ãštil para:
    - Ver historial de anÃ¡lisis
    - Monitorear actividad del sistema
    - Identificar patrones en resultados
    """
    if not db_pool:
        raise HTTPException(status_code=503, detail="Base de datos no disponible")
    
    try:
        async with db_pool.acquire() as conn:
            analyses = await conn.fetch("""
                SELECT 
                    id,
                    candidate_name,
                    region,
                    archaeological_probability,
                    anomaly_score,
                    result_type,
                    recommended_action,
                    environment_type,
                    confidence_level,
                    created_at
                FROM archaeological_candidate_analyses
                ORDER BY created_at DESC
                LIMIT $1
            """, limit)
            
            return {
                "total": len(analyses),
                "analyses": [
                    {
                        "id": row['id'],
                        "candidate_name": row['candidate_name'],
                        "region": row['region'],
                        "archaeological_probability": float(row['archaeological_probability']),
                        "anomaly_score": float(row['anomaly_score']),
                        "result_type": row['result_type'],
                        "recommended_action": row['recommended_action'],
                        "environment_type": row['environment_type'],
                        "confidence_level": float(row['confidence_level']),
                        "created_at": row['created_at'].isoformat()
                    }
                    for row in analyses
                ]
            }
    except Exception as e:
        print(f"[ERROR] Error consultando anÃ¡lisis: {e}", flush=True)
        raise HTTPException(status_code=500, detail=f"Error consultando anÃ¡lisis: {str(e)}")

@router.get("/analyses/{analysis_id}", summary="Obtener anÃ¡lisis por ID")
async def get_analysis_by_id(analysis_id: int):
    """
    # Consultar AnÃ¡lisis EspecÃ­fico
    
    Retorna un anÃ¡lisis completo por su ID, incluyendo todas las mediciones instrumentales asociadas.
    
    ## ParÃ¡metros
    
    - `analysis_id` (requerido): ID Ãºnico del anÃ¡lisis
    
    ## Respuesta
    
    Objeto con dos secciones:
    
    ### 1. Analysis
    - Datos completos del anÃ¡lisis cientÃ­fico
    - Probabilidades, scores, acciones recomendadas
    - Metadatos (ambiente, confianza, fecha)
    
    ### 2. Measurements
    - Lista de mediciones instrumentales
    - Nombre del instrumento (MODIS LST, ICESat-2, etc.)
    - Valor medido y unidad
    - Modo de datos (OK, DERIVED, SIMULATED)
    - Coordenadas y timestamp
    
    ## Errores
    
    - `404`: AnÃ¡lisis no encontrado
    - `503`: Base de datos no disponible
    """
    if not db_pool:
        raise HTTPException(status_code=503, detail="Base de datos no disponible")
    
    try:
        async with db_pool.acquire() as conn:
            # Obtener anÃ¡lisis
            analysis = await conn.fetchrow("""
                SELECT 
                    id,
                    candidate_name,
                    region,
                    archaeological_probability,
                    anomaly_score,
                    result_type,
                    recommended_action,
                    environment_type,
                    confidence_level,
                    created_at
                FROM archaeological_candidate_analyses
                WHERE id = $1
            """, analysis_id)
            
            if not analysis:
                raise HTTPException(status_code=404, detail=f"AnÃ¡lisis {analysis_id} no encontrado")
            
            # Obtener mediciones asociadas (por coordenadas cercanas y fecha cercana)
            # Usar CAST para convertir timestamp sin zona horaria a timestamp con zona horaria
            measurements = await conn.fetch("""
                SELECT 
                    instrument_name,
                    value,
                    unit,
                    data_mode,
                    latitude,
                    longitude,
                    measurement_timestamp
                FROM measurements
                WHERE measurement_timestamp >= ($1::timestamp - INTERVAL '1 hour')
                  AND measurement_timestamp <= ($1::timestamp + INTERVAL '1 hour')
                ORDER BY measurement_timestamp DESC
                LIMIT 20
            """, analysis['created_at'])
            
            return {
                "analysis": {
                    "id": analysis['id'],
                    "candidate_name": analysis['candidate_name'],
                    "region": analysis['region'],
                    "archaeological_probability": float(analysis['archaeological_probability']),
                    "anomaly_score": float(analysis['anomaly_score']),
                    "result_type": analysis['result_type'],
                    "recommended_action": analysis['recommended_action'],
                    "environment_type": analysis['environment_type'],
                    "confidence_level": float(analysis['confidence_level']),
                    "created_at": analysis['created_at'].isoformat()
                },
                "measurements": [
                    {
                        "instrument_name": row['instrument_name'],
                        "value": float(row['value']),
                        "unit": row['unit'],
                        "data_mode": row['data_mode'],
                        "latitude": float(row['latitude']),
                        "longitude": float(row['longitude']),
                        "timestamp": row['measurement_timestamp'].isoformat()
                    }
                    for row in measurements
                ]
            }
    except HTTPException:
        raise
    except Exception as e:
        print(f"[ERROR] Error consultando anÃ¡lisis {analysis_id}: {e}", flush=True)
        raise HTTPException(status_code=500, detail=f"Error consultando anÃ¡lisis: {str(e)}")

@router.get("/analyses/by-region/{region_name}", summary="Obtener anÃ¡lisis por regiÃ³n")
async def get_analyses_by_region(region_name: str, limit: int = 10):
    """
    # Consultar AnÃ¡lisis por RegiÃ³n
    
    Retorna todos los anÃ¡lisis realizados en una regiÃ³n especÃ­fica.
    
    ## ParÃ¡metros
    
    - `region_name` (requerido): Nombre de la regiÃ³n (ej: "Groenlandia Test", "Sahara Norte")
    - `limit` (opcional): NÃºmero mÃ¡ximo de anÃ¡lisis a retornar (default: 10)
    
    ## Respuesta
    
    Objeto con:
    - `region`: Nombre de la regiÃ³n consultada
    - `total`: NÃºmero de anÃ¡lisis encontrados
    - `analyses`: Lista de anÃ¡lisis ordenados por fecha
    
    ## Uso
    
    Ãštil para:
    - Comparar mÃºltiples anÃ¡lisis de la misma regiÃ³n
    - Evaluar cambios temporales
    - Validar consistencia de resultados
    - AnÃ¡lisis de series temporales
    
    ## Ejemplo
    
    ```
    GET /api/scientific/analyses/by-region/Groenlandia%20Test?limit=5
    ```
    """
    if not db_pool:
        raise HTTPException(status_code=503, detail="Base de datos no disponible")
    
    try:
        async with db_pool.acquire() as conn:
            analyses = await conn.fetch("""
                SELECT 
                    id,
                    candidate_name,
                    region,
                    archaeological_probability,
                    anomaly_score,
                    result_type,
                    recommended_action,
                    environment_type,
                    confidence_level,
                    created_at
                FROM archaeological_candidate_analyses
                WHERE region = $1
                ORDER BY created_at DESC
                LIMIT $2
            """, region_name, limit)
            
            return {
                "region": region_name,
                "total": len(analyses),
                "analyses": [
                    {
                        "id": row['id'],
                        "candidate_name": row['candidate_name'],
                        "archaeological_probability": float(row['archaeological_probability']),
                        "anomaly_score": float(row['anomaly_score']),
                        "result_type": row['result_type'],
                        "recommended_action": row['recommended_action'],
                        "environment_type": row['environment_type'],
                        "confidence_level": float(row['confidence_level']),
                        "created_at": row['created_at'].isoformat()
                    }
                    for row in analyses
                ]
            }
    except Exception as e:
        print(f"[ERROR] Error consultando anÃ¡lisis de regiÃ³n {region_name}: {e}", flush=True)
        raise HTTPException(status_code=500, detail=f"Error consultando anÃ¡lisis: {str(e)}")
